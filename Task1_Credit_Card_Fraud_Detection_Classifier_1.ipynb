{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nWdKMlMnrOi-",
        "EudJayPt3hnz",
        "T9Bh0H5AnKO1",
        "T6jH03SJhQ1x",
        "sUeotW2El1Sl"
      ],
      "toc_visible": true,
      "mount_file_id": "1z4jM6F-HazoVJXShIOLUPMaGK-oW5awt",
      "authorship_tag": "ABX9TyN7hDPJHcePBm2xK9tpTofR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ang-bill/IU-DLMDSME01-Credit-Card-Fraud-Detection/blob/main/Task1_Credit_Card_Fraud_Detection_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Classifier 1"
      ],
      "metadata": {
        "id": "C9SieIYotZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A. Retrieve Dataset from Kaggle Hub\n",
        "At the first run, the dataset is downloaded from Kaggle and stored locally. Subsequent runs check whether the file already exists.\n",
        "See: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
      ],
      "metadata": {
        "id": "nWdKMlMnrOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28d_gYwkpQn",
        "outputId": "06b6d66a-4aa8-4307-bb92-199ca3daed53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'creditcard.csv' found locally at '/content/drive/MyDrive/Colab_Kaggle_Data/mlg-ulb/creditcardfraud/creditcard.csv'. Loading from there.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd # Pandas dataframe\n",
        "import kagglehub # Kagglehub to access dataset\n",
        "import shutil # Util for copying files\n",
        "from google.colab import drive # Import Google Drive utilities\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "local_storage_base_dir = \"/content/drive/MyDrive/Colab_Kaggle_Data\"\n",
        "\n",
        "# Dataset details\n",
        "kaggle_dataset_id = \"mlg-ulb/creditcardfraud\"\n",
        "file_name_in_dataset = \"creditcard.csv\"\n",
        "\n",
        "# Construct the full path to locally stored dataset\n",
        "local_dataset_dir = os.path.join(local_storage_base_dir, *kaggle_dataset_id.split('/'))\n",
        "full_local_file_path = os.path.join(local_dataset_dir, file_name_in_dataset)\n",
        "\n",
        "# Ensure the desired local storage directory exists\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "df = None # Initialize pandas df\n",
        "\n",
        "# Check if the file already exists in local storage, otherwise download from Kaggle\n",
        "if os.path.exists(full_local_file_path):\n",
        "    print(f\"'{file_name_in_dataset}' found locally at '{full_local_file_path}'. Loading from there.\")\n",
        "else:\n",
        "    print(f\"'{file_name_in_dataset}' not found locally. Attempting to download from KaggleHub and store it.\")\n",
        "\n",
        "    # Use kagglehub.dataset_download to get the dataset.\n",
        "    downloaded_source_root = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "\n",
        "    # Construct the path to the file within the KaggleHub download location\n",
        "    source_file_path = os.path.join(downloaded_source_root, file_name_in_dataset)\n",
        "\n",
        "    if os.path.exists(source_file_path):\n",
        "        print(f\"Dataset found at KaggleHub resolved location: '{source_file_path}'.\")\n",
        "        print(f\"Copying '{file_name_in_dataset}' to local path: '{full_local_file_path}'.\")\n",
        "\n",
        "        # Copy the file to local storage location\n",
        "        shutil.copy(source_file_path, full_local_file_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Failed to find '{file_name_in_dataset}' at source '{source_file_path}' after KaggleHub download resolution.\")\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv(full_local_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B. Implementation of Classifier - Base\n"
      ],
      "metadata": {
        "id": "dulJshxnkvMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install dependencies\n",
        "(not included in default Colab Notebook)"
      ],
      "metadata": {
        "id": "LVQV_Xedpi9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uWLLZXfh48X7",
        "outputId": "825c202b-c135-43d2-8a8f-9b6f4e796e37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyod\n",
            "  Downloading pyod-2.0.6-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyod) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyod) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.12/dist-packages (from pyod) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.12/dist-packages (from pyod) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.6.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.17.0)\n",
            "Downloading pyod-2.0.6-py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyod\n",
            "Successfully installed pyod-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Customised Class on Feature Engineering\n",
        "\n",
        "\n",
        "*   HourExtractor: creates a new 'Hour' feature from 'Time'\n",
        "\n"
      ],
      "metadata": {
        "id": "JBnhr6oO0mtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HourExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts 'Hour' from 'Time' feature to capture diurnal patterns\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        # Convert seconds to hour of day (0-23)\n",
        "        if isinstance(X_copy, pd.DataFrame):\n",
        "            X_copy['Hour'] = (X_copy['Time'] % (60*60*24)) // (60*60)\n",
        "            #return X_copy.drop(columns=['Time']) # Replace Time with Hour\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "ktp61PnJ0DXx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "class FraudFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Comprehensive Feature Engineering for Credit Card Fraud.\n",
        "\n",
        "    Adds:\n",
        "    1. Time Features: Hour, Is_Night\n",
        "    2. Amount Features: Log_Amount, Amount_Decimal\n",
        "    3. Interaction Features: V14 * V17 (based on EDA correlations)\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Applies transformations to X.\"\"\"\n",
        "        X_out = X.copy()\n",
        "\n",
        "        # --- 1. Time Features ---\n",
        "        if 'Time' in X_out.columns:\n",
        "            # Hour (0-23)\n",
        "            X_out['Hour'] = (X_out['Time'] % 86400) // 3600\n",
        "\n",
        "            # Is_Night (Binary): 1 if between 22:00 and 06:00\n",
        "            # Helping trees isolate nocturnal fraud patterns instantly\n",
        "            X_out['Is_Night'] = (X_out['Hour'] >= 22) | (X_out['Hour'] <= 6)\n",
        "            X_out['Is_Night'] = X_out['Is_Night'].astype(int)\n",
        "\n",
        "        # --- 2. Amount Features ---\n",
        "        if 'Amount' in X_out.columns:\n",
        "            # Log Amount: Handles the extreme skew of transaction amounts\n",
        "            # np.log1p avoids log(0) errors\n",
        "            X_out['Log_Amount'] = np.log1p(X_out['Amount'])\n",
        "\n",
        "            # Decimal Part: Captures pricing psychology (e.g., 9.99 vs 10.00)\n",
        "            X_out['Amount_Dec'] = X_out['Amount'] % 1\n",
        "\n",
        "        # --- 3. Interaction Features (Based on EDA) ---\n",
        "        # V17 and V14 had the strongest correlations in your EDA.\n",
        "        # Multiplying them helps trees find the \"diagonal\" decision boundary.\n",
        "        if 'V14' in X_out.columns and 'V17' in X_out.columns:\n",
        "            X_out['V14_V17'] = X_out['V14'] * X_out['V17']\n",
        "\n",
        "        return X_out"
      ],
      "metadata": {
        "id": "KZq1yah71Jzn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Customised Class on Probability Calibration\n",
        "A customised classifier applies analytical probability calibration according to Dal Pozzolo et al. (2025). This approach enables integration with the scikit-learn library.\n",
        "\n",
        "https://doi.org/10.1109/SSCI.2015.33"
      ],
      "metadata": {
        "id": "EudJayPt3hnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import numpy as np\n",
        "\n",
        "class PozzoloCalibratedClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Wraps a classifier to apply Dal Pozzolo's prior correction automatically\n",
        "    during prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object\n",
        "        The base classifier (e.g., LogisticRegression or XGBClassifier).\n",
        "    original_prior : float\n",
        "        The prevalence of the positive class in the original dataset (e.g., 0.00172).\n",
        "    sampling_ratio : float, default=1.0\n",
        "        The target ratio used in RandomUnderSampler (1.0 means 50/50).\n",
        "    \"\"\"\n",
        "\n",
        "    _estimator_type = \"classifier\"\n",
        "\n",
        "    def __init__(self, estimator, original_prior=0.00172, sampling_ratio=1.0):\n",
        "        self.estimator = estimator\n",
        "        self.original_prior = original_prior\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Fit the internal model on the data provided (which is already RUS-sampled)\n",
        "        self.estimator.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        check_is_fitted(self.estimator)\n",
        "\n",
        "        # 1. Get Biased Probabilities (P_s) from the internal model\n",
        "        # The model thinks the world is 50% fraud because of RUS\n",
        "        probs_biased = self.estimator.predict_proba(X)\n",
        "\n",
        "        # If we only have 1 class in test (edge case), return as is\n",
        "        if probs_biased.shape[1] != 2:\n",
        "            return probs_biased\n",
        "\n",
        "        p_s = probs_biased[:, 1]\n",
        "\n",
        "        # 2. Calculate Correction Factor (Gamma)\n",
        "        # Gamma = (Original_Odds) / (Sampled_Odds)\n",
        "        # Sampled_Odds for ratio 1.0 is 0.5/0.5 = 1\n",
        "        prior_s = self.sampling_ratio / (1 + self.sampling_ratio) # e.g. 0.5\n",
        "\n",
        "        # Edge case protection\n",
        "        if self.original_prior <= 0 or self.original_prior >= 1:\n",
        "            return probs_biased\n",
        "\n",
        "        gamma = (self.original_prior / (1 - self.original_prior)) / \\\n",
        "                (prior_s / (1 - prior_s))\n",
        "\n",
        "        # 3. Apply Formula\n",
        "        p_calib = (gamma * p_s) / (gamma * p_s + (1 - p_s))\n",
        "\n",
        "        # Return in Scikit-Learn format [P(0), P(1)]\n",
        "        return np.vstack([1 - p_calib, p_calib]).T\n",
        "\n",
        "    def predict(self, X):\n",
        "        # This is the Key: predict() now uses the CALIBRATED probability\n",
        "        # So F2 Score optimization sees the real-world performance\n",
        "        probs = self.predict_proba(X)[:, 1]\n",
        "        return (probs > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Necessary for RandomizedSearchCV to access the inner 'estimator' params\n",
        "        params = super().get_params(deep)\n",
        "        if deep and hasattr(self.estimator, 'get_params'):\n",
        "            for key, value in self.estimator.get_params().items():\n",
        "                params[f'estimator__{key}'] = value\n",
        "        return params"
      ],
      "metadata": {
        "id": "lcOPOTgz40Op"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Customised Class TOS Generator for XGBOD"
      ],
      "metadata": {
        "id": "T9Bh0H5AnKO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TOSGenerator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Generates Transformed Outlier Scores (TOS) for XGBOD.\n",
        "    Wraps PyOD unsupervised detectors to act as feature extractors.\n",
        "    \"\"\"\n",
        "    def __init__(self, detectors=None):\n",
        "        self.detectors = detectors\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # If no detectors provided, initialize default ones\n",
        "        # Note: We initialize here to ensure fresh models for every fold\n",
        "        if self.detectors is None:\n",
        "            self.detectors = [KNN(n_neighbors=5), LOF(n_neighbors=5)]\n",
        "\n",
        "        for detector in self.detectors:\n",
        "            detector.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        tos_features = []\n",
        "        for detector in self.detectors:\n",
        "            # For PyOD, decision_function returns the anomaly score\n",
        "            # Reshape to (n_samples, 1)\n",
        "            scores = detector.decision_function(X).reshape(-1, 1)\n",
        "            tos_features.append(scores)\n",
        "\n",
        "        # Stack original features with new TOS features\n",
        "        return np.hstack([X] + tos_features)"
      ],
      "metadata": {
        "id": "eoxg-9INnDfM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Required Packages"
      ],
      "metadata": {
        "id": "AS--3bgHqBCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.base import clone, BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, fbeta_score, f1_score, precision_score, recall_score, brier_score_loss\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Supports resampling inside CV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "import warnings\n",
        "# Filter specific warning from imblearn pipeline (supress as pipeline works correctly)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"imblearn.pipeline\")"
      ],
      "metadata": {
        "id": "ciTsNFN8z7tI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Configuration of pipelines\n",
        "The ablation study evaluates the effect of each component separately, therefore distinct pipelines are defined.\n",
        "\n",
        "**Components:**\n",
        "*   Feature engineering: Augmentation with Hour vs. no feature engineering\n",
        "*   Data preprocessing: RobustScaler vs. no scaling\n",
        "*   Resampling: RUS vs. no resampling\n",
        "*   Classifier optimisation: Randomised parameter optimisation vs. using default parameters\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p41hJo3O6KkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define distinct pipelines to test each component\n",
        "configs = {\n",
        "    \"1. Naive (No Preproc)\": {\n",
        "        'scale': False, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"2. + Scaling (Robust)\": {\n",
        "        'scale': True, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3. + Feature Eng (Hour)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"4. + RUS (Calibrated)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': False\n",
        "    },\n",
        "    \"5. + Optimization (with RUS)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': True\n",
        "    },\n",
        "    \"6. + Optimization (without RUS)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': True\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4efdI6Mp6Wtr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create features and labels"
      ],
      "metadata": {
        "id": "ELWDXcqNCKIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP: Synthetic Data mimicking your EDA findings ---\n",
        "#from sklearn.datasets import make_classification\n",
        "# We generate data with 'Time' (0-172800 seconds) and 'Amount' features\n",
        "#N_SAMPLES = 5000\n",
        "#X, y = make_classification(n_samples=N_SAMPLES, n_features=28, n_informative=20,\n",
        "#                           weights=[0.99828, 0.00172], # 0.172% minority\n",
        "#                           random_state=42)\n",
        "\n",
        "# Create DataFrame to simulate real columns\n",
        "#cols = [f'V{i}' for i in range(1, 29)]\n",
        "#df_X = pd.DataFrame(X, columns=cols)\n",
        "# Add 'Time' (0 to 48 hours in seconds) and 'Amount' (with outliers)\n",
        "#df_X['Time'] = np.random.randint(0, 172800, size=N_SAMPLES)\n",
        "#df_X['Amount'] = np.random.exponential(scale=100, size=N_SAMPLES)\n",
        "#X = df_X # Use DataFrame for the pipeline\n",
        "X = df.drop('Class', axis=1)  # features\n",
        "y = df['Class'] # Labels\n",
        "original_fraud_rate = np.mean(y)\n",
        "\n",
        "print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Dataset Shape: {X.shape}, Fraud Ratio: {np.mean(y):.4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx5oGgFEHCL",
        "outputId": "ca534f76-1778-45b5-ae46-e7e2f42d8539"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-08 21:39:56 Dataset Shape: (284807, 30), Fraud Ratio: 0.001727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Run Ablation Study (Train, Validate, and Test)"
      ],
      "metadata": {
        "id": "vGMQm6JvEYbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ablation_study(experiment_name, base_clf, param_dist, X, y, configs, extra_pipeline_steps=None):\n",
        "    \"\"\"\n",
        "    Runs the full ablation study for a specific classifier.\n",
        "\n",
        "    Parameters:\n",
        "    - experiment_name: String label (e.g., \"XGBoost\")\n",
        "    - base_clf: The instantiated classifier object (e.g., XGBClassifier())\n",
        "    - param_dist: Dictionary for RandomizedSearchCV\n",
        "    - X, y: Data\n",
        "    - configs: Dictionary of ablation configurations\n",
        "    - extra_pipeline_steps: List of (name, transformer) tuples to add before scaling (e.g., TOS)\n",
        "    \"\"\"\n",
        "\n",
        "    # Cross Validation Outer Loop: Repeated Stratified 5-Fold\n",
        "    #outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "    # Cross Validation Inner Loop: Stratified 4-Fold (used inside RandomizedSearchCV)\n",
        "    inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    print(f\"--- Starting Experiment: {experiment_name} ---\")\n",
        "\n",
        "    for name, cfg in configs.items():\n",
        "        print(f\"\\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Running Configuration: {name}\")\n",
        "\n",
        "        fold_metrics = {'f2': [], 'f1': [], 'rec': [], 'prec': [], 'best_params': []}\n",
        "\n",
        "        # Outer CV loop (split training and test set)\n",
        "        for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # --- A. Build Pipeline Steps ---\n",
        "            steps = []\n",
        "\n",
        "            # 1. Feature Engineering (Hour)\n",
        "            if cfg['fe']:\n",
        "                #steps.append(('fe', HourExtractor()))\n",
        "                steps.append(('fe', FraudFeatureEngineer()))\n",
        "            #else:\n",
        "                # Drop Time if not using FE (standard practice if raw Time is not useful)\n",
        "            #    steps.append(('drop_time', ColumnTransformer([('drop', 'drop', ['Time'])], remainder='passthrough')))\n",
        "\n",
        "            # 1b. Extra Steps (e.g., XGBOD TOS Generator)\n",
        "            if extra_pipeline_steps:\n",
        "                steps.extend(extra_pipeline_steps)\n",
        "\n",
        "            # 2. Scaling (RobustScaler)\n",
        "            if cfg['scale']:\n",
        "                # Apply robust scaler to Amount, pass through others\n",
        "                # @TODO\n",
        "                # Note: For simplicity in this demo, we apply to all numericals coming out of previous step\n",
        "                steps.append(('scaler', RobustScaler()))\n",
        "\n",
        "            # 3. Resampling (RUS)\n",
        "            # Resampling in the pipeline preventes data leakage\n",
        "            # Resampling is only applied to the traning fold inside\n",
        "            # (https://imbalanced-learn.org/stable/common_pitfalls.html)\n",
        "            if cfg['rus']:\n",
        "                steps.append(('rus', RandomUnderSampler(sampling_strategy=1.0, random_state=42)))\n",
        "\n",
        "            # 4. Classifier\n",
        "            # SciKit-Learn LogisticRegression\n",
        "            # (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "            # Regularization is applied by default\n",
        "            # Solver liblinear: supports L1 and L2 regularization\n",
        "            if cfg['rus']:\n",
        "                  #base_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "                  #final_clf = PozzoloCalibratedClassifier(\n",
        "                  #    estimator=base_clf,\n",
        "                  #    original_prior=original_fraud_rate,\n",
        "                  #    sampling_ratio=1.0\n",
        "                  #)\n",
        "                  # Wrap with Pozzolo Calibration\n",
        "                  final_clf = PozzoloCalibratedClassifier(\n",
        "                      estimator=clone(base_clf), # clone base_clf to ensure fresh start\n",
        "                      original_prior=original_fraud_rate,\n",
        "                      sampling_ratio=1.0\n",
        "                  )\n",
        "            else:\n",
        "                  #final_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "                  final_clf = clone(base_clf)\n",
        "\n",
        "            steps.append(('clf', final_clf))\n",
        "\n",
        "            # Create pipeline from steps\n",
        "            pipeline = ImbPipeline(steps)\n",
        "\n",
        "            # --- B. Optimization (Inner Loop) ---\n",
        "            if cfg['opt']:\n",
        "                # Inner CV for optimisation\n",
        "                # (CV uses a fold of the train set for validation)\n",
        "                # Optimize for F2 Score\n",
        "\n",
        "                # Define distribution of the tuneable parameters\n",
        "                # print(clf.get_params()) # Print tunable parameters\n",
        "                # Naming convention of parameter names: stepname__parameter\n",
        "                # Here, the clf, the classifier of the pipeline is tuned\n",
        "                #clf_param_name = 'clf__estimator__C' if cfg['rus'] else 'clf__C'\n",
        "                #param_dist = {\n",
        "                #    clf_param_name: loguniform(1e-4, 1e2) # Inverse of regularization strength\n",
        "                #}\n",
        "\n",
        "                iterations = 50 if cfg['rus'] else 15\n",
        "\n",
        "                # Adjust param names for wrapper vs standalone\n",
        "                # If wrapped: clf__estimator__param\n",
        "                # If standalone: clf__param\n",
        "                prefix = 'clf__estimator__' if cfg['rus'] else 'clf__'\n",
        "\n",
        "                # Update param_dist keys to match current pipeline structure\n",
        "                tuned_params = {f\"{prefix}{k}\": v for k, v in param_dist.items()}\n",
        "\n",
        "                search = RandomizedSearchCV(pipeline, tuned_params,\n",
        "                                            n_iter=iterations,\n",
        "                                            scoring=make_scorer(fbeta_score, beta=2),\n",
        "                                            cv=inner_cv, n_jobs=-1,\n",
        "                                            random_state=42)\n",
        "                search.fit(X_train, y_train)\n",
        "                model = search.best_estimator_\n",
        "            else:\n",
        "                pipeline.fit(X_train, y_train)\n",
        "                model = pipeline\n",
        "\n",
        "            # --- C. Prediction & Calibration ---\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # --- D. Record Metrics ---\n",
        "            fold_metrics['f2'].append(fbeta_score(y_test, y_pred, beta=2))\n",
        "            fold_metrics['f1'].append(f1_score(y_test, y_pred))\n",
        "            fold_metrics['rec'].append(recall_score(y_test, y_pred))\n",
        "            fold_metrics['prec'].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "            if cfg['opt']and len(fold_metrics['best_params']) > 0 :\n",
        "              fold_metrics['best_params'].append(search.best_params_)\n",
        "              params_df = pd.DataFrame(search.best_params_)\n",
        "\n",
        "        # --- Aggregate results for this configuration ---\n",
        "\n",
        "        # Extract all C values found - key 'clf__estimator__C'\n",
        "        #C_values_found = []\n",
        "        #for params in fold_metrics['best_params']:\n",
        "        #    if 'clf__estimator__C' in params:\n",
        "        #        C_value = params['clf__estimator__C']\n",
        "        #        C_values_found.append(C_value)\n",
        "        #\n",
        "\n",
        "        # add to result table\n",
        "        #results_table.append({\n",
        "        result_row = {\n",
        "            'Configuration': name,\n",
        "            'F2 Score (Mean)': f\"{np.mean(fold_metrics['f2']):.4f}\",\n",
        "            'F2 Score (SD)': f\"{np.std(fold_metrics['f2']):.4f}\",\n",
        "            'F1 Score (Mean)': f\"{np.mean(fold_metrics['f1']):.4f}\",\n",
        "            'F1 Score (SD)': f\"{np.std(fold_metrics['f1']):.4f}\",\n",
        "            'F2 Recall (Mean)': f\"{np.mean(fold_metrics['rec']):.4f}\",\n",
        "            'F2 Recall (SD)': f\"{np.std(fold_metrics['rec']):.4f}\",\n",
        "            'F1 Precision (Mean)': f\"{np.mean(fold_metrics['prec']):.4f}\",\n",
        "            'F1 Precision (SD)': f\"{np.std(fold_metrics['prec']):.4f}\"\n",
        "        }\n",
        "        #    })\n",
        "        #if len(C_values_found) > 0:\n",
        "        #  results_table.append({\n",
        "        #    'Best Params (Mean)': f\"{np.mean(C_values_found):.4f}\",\n",
        "        #    'Best Params (SD)': f\"{np.std(C_values_found):.4f}\"\n",
        "        #    })\n",
        "\n",
        "        # Add hyperparameters\n",
        "        # Calculate mean and sd for hyperparameters\n",
        "        params_df = pd.DataFrame(fold_metrics['best_params'])\n",
        "        numeric_params_df = params_df.select_dtypes(include=[np.number])\n",
        "        mean_params = numeric_params_df.mean()\n",
        "        sd_params = numeric_params_df.std(ddof=1) # Use ddof=1 for sample SD\n",
        "\n",
        "        for param_name in mean_params.index:\n",
        "            mean_val = mean_params[param_name]\n",
        "            sd_val = sd_params[param_name]\n",
        "\n",
        "            # Use a clean, generic column name (e.g., C_Mean)\n",
        "            base_name = param_name.split('__')[-1] # Extracts 'C' from 'clf__C'\n",
        "\n",
        "            result_row[f'{base_name} (Mean)'] = f\"{mean_val:.4f}\"\n",
        "            result_row[f'{base_name} (SD)'] = f\"{sd_val:.4f}\"\n",
        "\n",
        "        # 5. Append the complete row to the results table\n",
        "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        print(pd.DataFrame([result_row]).to_markdown(index=False))\n",
        "        results_table.append(result_row)\n",
        "\n",
        "    # --- Return result ---\n",
        "    return pd.DataFrame(results_table)"
      ],
      "metadata": {
        "id": "8Zj1FHRsaryp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2C. Implementation of Base Classifier - Logistic Regression\n"
      ],
      "metadata": {
        "id": "T6jH03SJhQ1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_params = {'C': loguniform(1e-4, 1e2)}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"Logistic Regression\",\n",
        "    base_clf=LogisticRegression(solver='liblinear', random_state=42),\n",
        "    param_dist=lr_params,\n",
        "    X=X, y=y, configs=configs\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: Logistic Regression) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLBnB6phmEP",
        "outputId": "93c73a1b-54a2-4f3c-a423-827c2efeaa7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Experiment: Logistic Regression ---\n",
            "Running Configuration: 1. Naive (No Preproc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Configuration: 2. + Scaling (Robust)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Configuration: 3. + Feature Eng (Hour)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Configuration: 4. + RUS (Calibrated)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Configuration: 5. + Optimization (Full)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
            "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |   Best Params (Mean) |   Best Params (SD) |\n",
            "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|---------------------:|-------------------:|\n",
            "| 1. Naive (No Preproc)    |            0.6195 |          0.0538 |            0.6599 |          0.0434 |             0.5959 |           0.0615 |                0.7465 |              0.0574 |             nan      |           nan      |\n",
            "| 2. + Scaling (Robust)    |            0.6569 |          0.0464 |            0.7238 |          0.0364 |             0.6191 |           0.0516 |                0.8756 |              0.0299 |             nan      |           nan      |\n",
            "| 3. + Feature Eng (Hour)  |            0.6576 |          0.0444 |            0.7249 |          0.0344 |             0.6195 |           0.0496 |                0.8778 |              0.0297 |             nan      |           nan      |\n",
            "| 4. + RUS (Calibrated)    |            0.6978 |          0.0669 |            0.5995 |          0.1059 |             0.7967 |           0.0586 |                0.4988 |              0.1465 |             nan      |           nan      |\n",
            "| 5. + Optimization (Full) |            0.7008 |          0.0582 |            0.6215 |          0.0894 |             0.774  |           0.0573 |                0.5343 |              0.1329 |               0.5219 |             0.2571 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 with PozzoloCalibratedClassifier (after code changes):\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |   Best Params (Mean) |   Best Params (SD) |\n",
        "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|---------------------:|-------------------:|\n",
        "| 1. Naive (No Preproc)    |            0.6195 |          0.0538 |            0.6599 |          0.0434 |             0.5959 |           0.0615 |                0.7465 |              0.0574 |             nan      |           nan      |\n",
        "| 2. + Scaling (Robust)    |            0.6569 |          0.0464 |            0.7238 |          0.0364 |             0.6191 |           0.0516 |                0.8756 |              0.0299 |             nan      |           nan      |\n",
        "| 3. + Feature Eng (Hour)  |            0.6576 |          0.0444 |            0.7249 |          0.0344 |             0.6195 |           0.0496 |                0.8778 |              0.0297 |             nan      |           nan      |\n",
        "| 4. + RUS (Calibrated)    |            0.6978 |          0.0669 |            0.5995 |          0.1059 |             0.7967 |           0.0586 |                0.4988 |              0.1465 |             nan      |           nan      |\n",
        "| 5. + Optimization (Full) |            0.7008 |          0.0582 |            0.6215 |          0.0894 |             0.774  |           0.0573 |                0.5343 |              0.1329 |               0.5219 |             0.2571 |\n"
      ],
      "metadata": {
        "id": "GqhqVGVDypw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 with PozzoloCalibratedClassifier :\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.7008 ± 0.0582        |     0.6215 |   0.774  |      0.5343 |\n"
      ],
      "metadata": {
        "id": "6AwBaeSVAYc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 without PozzoloCalibratedClassifier:\n",
        "\n",
        "\n",
        "```\n",
        "Dataset Shape: (284807, 30), Fraud Ratio: 0.1727%\n",
        "Starting Ablation Study (this may take a moment)...\n",
        "Running Configuration: 1. Naive (No Preproc)\n",
        "Running Configuration: 2. + Scaling (Robust)\n",
        "Running Configuration: 3. + Feature Eng (Hour)\n",
        "Running Configuration: 4. + RUS (Calibrated)\n",
        "Running Configuration: 5. + Optimization (Full)\n",
        "```\n",
        "\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.6052 ± 0.0515        |     0.6251 |   0.5946 |      0.6721 |"
      ],
      "metadata": {
        "id": "Vz0aY15t47E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2D. Implementation of Classifier 1 - XGBoost\n"
      ],
      "metadata": {
        "id": "0MTNoylRlKM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint, uniform, loguniform\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGBoost Parameters: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "# Params version 1\n",
        "xgb_params_v1 = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': loguniform(0.01, 0.3)\n",
        "}\n",
        "\n",
        "# Params version 2\n",
        "xgb_params_v2 = {\n",
        "    'scale_pos_weight': randint(10, 101),  # Range: 10 - 100\n",
        "    'max_depth': randint(3, 7),  # Range: 3 - 6 (randint upper bound is exclusive)\n",
        "    'learning_rate': loguniform(0.01, 0.1),  # Range: 0.01 - 0.1\n",
        "    'n_estimators': [100, 200],\n",
        "    'min_child_weight': randint(5, 11),    # Range: 5 - 10\n",
        "    'gamma': uniform(0.1, 0.4),            # Range: 0.1 - 0.5 (loc=0.1, scale=0.4)\n",
        "    'subsample': uniform(0.6, 0.3),        # Range: 0.6 - 0.9 (loc=0.6, scale=0.3)\n",
        "    'colsample_bytree': uniform(0.6, 0.3)  # Range: 0.6 - 0.9 (loc=0.6, scale=0.3)\n",
        "}\n",
        "\n",
        "# Params version 3\n",
        "xgb_params_v3 = {\n",
        "    'scale_pos_weight': randint(10, 101),  # Range: 10 - 100\n",
        "    'max_depth': randint(3, 7),  # Range: 3 - 6 (randint upper bound is exclusive)\n",
        "    'learning_rate': loguniform(0.01, 0.1),  # Range: 0.01 - 0.1\n",
        "\n",
        "    'n_estimators': [150],\n",
        "    'min_child_weight': [5],\n",
        "    'gamma': [0.1],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"XGBoost\",\n",
        "    base_clf=XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=1),\n",
        "      # eval_metric='logloss' - eval metric for learning, logloss (negative log-likelihood, suitable for classification)\n",
        "    param_dist=xgb_params_v3,\n",
        "    X=X, y=y, configs=configs\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: XGBoost) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NtDUHhClbo0",
        "outputId": "bb5977dc-4bd7-46b4-e0e1-6f5f9d0aab82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Experiment: XGBoost ---\n",
            "2025-12-08 21:58:12 Running Configuration: 1. Naive (No Preproc)\n",
            "2025-12-08 21:58:38\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 1. Naive (No Preproc) |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                 0.897 |              0.0386 |\n",
            "2025-12-08 21:58:38 Running Configuration: 2. + Scaling (Robust)\n",
            "2025-12-08 21:59:05\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 2. + Scaling (Robust) |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
            "2025-12-08 21:59:05 Running Configuration: 3. + Feature Eng (Hour)\n",
            "2025-12-08 21:59:39\n",
            "\n",
            "| Configuration           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 3. + Feature Eng (Hour) |            0.7959 |          0.0399 |            0.8342 |          0.0413 |             0.7723 |           0.0399 |                0.9076 |              0.0495 |\n",
            "2025-12-08 21:59:39 Running Configuration: 4. + RUS (Calibrated)\n",
            "2025-12-08 21:59:43\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 4. + RUS (Calibrated) |            0.7563 |          0.0367 |            0.7234 |          0.0781 |             0.7846 |           0.0169 |                0.6841 |              0.1264 |\n",
            "2025-12-08 21:59:43 Running Configuration: 5. + Optimization (with RUS)\n",
            "2025-12-08 22:12:15\n",
            "\n",
            "| Configuration                |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:-----------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 5. + Optimization (with RUS) |             0.781 |          0.0284 |            0.7696 |          0.0566 |             0.7907 |           0.0215 |                0.7574 |               0.105 |\n",
            "2025-12-08 22:12:15 Running Configuration: 6. + Optimization (without RUS)\n",
            "2025-12-08 22:34:41\n",
            "\n",
            "| Configuration                   |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:--------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 6. + Optimization (without RUS) |            0.8403 |          0.0193 |            0.8576 |          0.0273 |             0.8293 |           0.0164 |                 0.889 |              0.0458 |\n",
            "\n",
            "=== Ablation Study Results (Baseline: XGBoost) ===\n",
            "| Configuration                   |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:--------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 1. Naive (No Preproc)           |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                0.897  |              0.0386 |\n",
            "| 2. + Scaling (Robust)           |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
            "| 3. + Feature Eng (Hour)         |            0.7959 |          0.0399 |            0.8342 |          0.0413 |             0.7723 |           0.0399 |                0.9076 |              0.0495 |\n",
            "| 4. + RUS (Calibrated)           |            0.7563 |          0.0367 |            0.7234 |          0.0781 |             0.7846 |           0.0169 |                0.6841 |              0.1264 |\n",
            "| 5. + Optimization (with RUS)    |            0.781  |          0.0284 |            0.7696 |          0.0566 |             0.7907 |           0.0215 |                0.7574 |              0.105  |\n",
            "| 6. + Optimization (without RUS) |            0.8403 |          0.0193 |            0.8576 |          0.0273 |             0.8293 |           0.0164 |                0.889  |              0.0458 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Starting Experiment: XGBoost ---\n",
        "\n",
        "|          |  |   |   |    |   |   |    |    |\n",
        "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "2025-12-08 21:58:12 Running Configuration: 1. Naive (No Preproc)\n",
        "2025-12-08 21:58:38\n",
        "\n",
        "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc) |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                 0.897 |              0.0386 |\n",
        "|\n",
        "2025-12-08 21:58:38 Running Configuration: 2. + Scaling (Robust)\n",
        "2025-12-08 21:59:05\n",
        "\n",
        "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 2. + Scaling (Robust) |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
        "|\n",
        "2025-12-08 21:59:05 Running Configuration: 3. + Feature Eng (Hour)\n",
        "2025-12-08 21:59:39\n",
        "\n",
        "| Configuration           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 3. + Feature Eng (Hour) |            0.7959 |          0.0399 |            0.8342 |          0.0413 |             0.7723 |           0.0399 |                0.9076 |              0.0495 |\n",
        "|\n",
        "2025-12-08 21:59:39 Running Configuration: 4. + RUS (Calibrated)\n",
        "2025-12-08 21:59:43\n",
        "\n",
        "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 4. + RUS (Calibrated) |            0.7563 |          0.0367 |            0.7234 |          0.0781 |             0.7846 |           0.0169 |                0.6841 |              0.1264 |\n",
        "|\n",
        "2025-12-08 21:59:43 Running Configuration: 5. + Optimization (with RUS)\n",
        "2025-12-08 22:12:15\n",
        "\n",
        "| Configuration                |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:-----------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 5. + Optimization (with RUS) |             0.781 |          0.0284 |            0.7696 |          0.0566 |             0.7907 |           0.0215 |                0.7574 |               0.105 |\n",
        "|\n",
        "2025-12-08 22:12:15 Running Configuration: 6. + Optimization (without RUS)\n",
        "2025-12-08 22:34:41\n",
        "\n",
        "| Configuration                   |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:--------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 6. + Optimization (without RUS) |            0.8403 |          0.0193 |            0.8576 |          0.0273 |             0.8293 |           0.0164 |                 0.889 |              0.0458 |\n",
        "\n",
        "=== Ablation Study Results (Baseline: XGBoost) ===\n",
        "| Configuration                   |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:--------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)           |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                0.897  |              0.0386 |\n",
        "| 2. + Scaling (Robust)           |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
        "| 3. + Feature Eng (Hour)         |            0.7959 |          0.0399 |            0.8342 |          0.0413 |             0.7723 |           0.0399 |                0.9076 |              0.0495 |\n",
        "| 4. + RUS (Calibrated)           |            0.7563 |          0.0367 |            0.7234 |          0.0781 |             0.7846 |           0.0169 |                0.6841 |              0.1264 |\n",
        "| 5. + Optimization (with RUS)    |            0.781  |          0.0284 |            0.7696 |          0.0566 |             0.7907 |           0.0215 |                0.7574 |              0.105  |\n",
        "| 6. + Optimization (without RUS) |            0.8403 |          0.0193 |            0.8576 |          0.0273 |             0.8293 |           0.0164 |                0.889  |              0.0458 |"
      ],
      "metadata": {
        "id": "JbxB5NRmtAQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VC_ruGSvD2El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 (Params version 1, only 2 repeats):\n",
        "\n",
        "=== Ablation Study Results (Baseline: XGBoost) ===\n",
        "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)    |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                0.897  |              0.0386 |\n",
        "| 2. + Scaling (Robust)    |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
        "| 3. + Feature Eng (Hour)  |            0.7995 |          0.0253 |            0.8371 |          0.0294 |             0.7765 |           0.0246 |                0.9088 |              0.0443 |\n",
        "| 4. + RUS (Calibrated)    |            0.7695 |          0.0426 |            0.7339 |          0.0791 |             0.7988 |           0.0148 |                0.6885 |              0.1192 |\n",
        "| 5. + Optimization (Full) |            0.7619 |          0.0351 |            0.7321 |          0.0698 |             0.7866 |           0.0275 |                0.6959 |              0.1219 |\n"
      ],
      "metadata": {
        "id": "ikAouZIC9RZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2D. Implementation of Classifier 2 - XGBOD\n"
      ],
      "metadata": {
        "id": "sUeotW2El1Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOD is essentially XGBoost + TOS Features\n",
        "# So we use the same base_clf and params as XGBoost, but add the feature generator\n",
        "\n",
        "# XGBoost Parameters: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': loguniform(0.01, 0.3)\n",
        "}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"XGBOD (XGBoost + TOS)\",\n",
        "    base_clf=XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=1),\n",
        "      # eval_metric='logloss' - eval metric for learning, logloss (negative log-likelihood, suitable for classification)\n",
        "    param_dist=xgb_params,\n",
        "    X=X, y=y, configs=configs,\n",
        "    extra_pipeline_steps=[('tos', TOSGenerator())] # <--- This injects the Outlier Scores\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: XGBOD) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Q1XubgObmFC5",
        "outputId": "792aa782-157a-4dde-9f97-8d76c24fa365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TOSGenerator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-225491100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mparam_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mextra_pipeline_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOSGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# <--- This injects the Outlier Scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TOSGenerator' is not defined"
          ]
        }
      ]
    }
  ]
}