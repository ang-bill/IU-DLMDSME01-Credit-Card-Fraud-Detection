{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nWdKMlMnrOi-"
      ],
      "authorship_tag": "ABX9TyOjtVi/pYahJQ+wJSC7vXXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ang-bill/IU-DLMDSME01-Credit-Card-Fraud-Detection/blob/main/Task1_Credit_Card_Fraud_Detection_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Classifier 1"
      ],
      "metadata": {
        "id": "C9SieIYotZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A. Retrieve Dataset from Kaggle Hub\n",
        "At the first run, the dataset is downloaded from Kaggle and stored locally. Subsequent runs check whether the file already exists.\n",
        "See: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
      ],
      "metadata": {
        "id": "nWdKMlMnrOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28d_gYwkpQn",
        "outputId": "d64ee4e1-18a4-4822-9652-a460665bac17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'creditcard.csv' found locally at '/content/drive/MyDrive/Colab_Kaggle_Data/mlg-ulb/creditcardfraud/creditcard.csv'. Loading from there.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd # Pandas dataframe\n",
        "import kagglehub # Kagglehub to access dataset\n",
        "import shutil # Util for copying files\n",
        "from google.colab import drive # Import Google Drive utilities\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "local_storage_base_dir = \"/content/drive/MyDrive/Colab_Kaggle_Data\"\n",
        "\n",
        "# Dataset details\n",
        "kaggle_dataset_id = \"mlg-ulb/creditcardfraud\"\n",
        "file_name_in_dataset = \"creditcard.csv\"\n",
        "\n",
        "# Construct the full path to locally stored dataset\n",
        "local_dataset_dir = os.path.join(local_storage_base_dir, *kaggle_dataset_id.split('/'))\n",
        "full_local_file_path = os.path.join(local_dataset_dir, file_name_in_dataset)\n",
        "\n",
        "# Ensure the desired local storage directory exists\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "df = None # Initialize pandas df\n",
        "\n",
        "# Check if the file already exists in local storage, otherwise download from Kaggle\n",
        "if os.path.exists(full_local_file_path):\n",
        "    print(f\"'{file_name_in_dataset}' found locally at '{full_local_file_path}'. Loading from there.\")\n",
        "else:\n",
        "    print(f\"'{file_name_in_dataset}' not found locally. Attempting to download from KaggleHub and store it.\")\n",
        "\n",
        "    # Use kagglehub.dataset_download to get the dataset.\n",
        "    downloaded_source_root = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "\n",
        "    # Construct the path to the file within the KaggleHub download location\n",
        "    source_file_path = os.path.join(downloaded_source_root, file_name_in_dataset)\n",
        "\n",
        "    if os.path.exists(source_file_path):\n",
        "        print(f\"Dataset found at KaggleHub resolved location: '{source_file_path}'.\")\n",
        "        print(f\"Copying '{file_name_in_dataset}' to local path: '{full_local_file_path}'.\")\n",
        "\n",
        "        # Copy the file to local storage location\n",
        "        shutil.copy(source_file_path, full_local_file_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Failed to find '{file_name_in_dataset}' at source '{source_file_path}' after KaggleHub download resolution.\")\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv(full_local_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B. Implementation of Classifier 1\n"
      ],
      "metadata": {
        "id": "dulJshxnkvMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Required Packages\n",
        "(not included in default Colab Notebook)"
      ],
      "metadata": {
        "id": "LVQV_Xedpi9F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3b843db2",
        "outputId": "2dbd0ced-9056-46ac-fd76-28f82f3b5fc4"
      },
      "source": [
        "pip install pyod"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyod\n",
            "  Downloading pyod-2.0.6-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyod) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyod) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.12/dist-packages (from pyod) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.12/dist-packages (from pyod) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.6.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.17.0)\n",
            "Downloading pyod-2.0.6-py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyod\n",
            "Successfully installed pyod-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Required Packages\n",
        "(not included in default Colab Notebook)"
      ],
      "metadata": {
        "id": "AS--3bgHqBCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X = df.drop('Class', axis=1)  # features\n",
        "y = df['Class'] # Labels\n",
        "\n",
        "print(f\"Dataset Shape: {X.shape}, Fraud Ratio: {np.mean(y):.4%}\")\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "   ('clf', LogisticRegression())])\n",
        "pipe.fit(X, y)\n",
        "pipe[:-1].get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwnI61p3mxew",
        "outputId": "365a4bc1-b1e1-40a9-a1ce-2cc79dcb2f34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['x2', 'x3'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, fbeta_score, f1_score, precision_score, recall_score, brier_score_loss\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Supports resampling inside CV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# --- 1. SETUP: Synthetic Data mimicking your EDA findings ---\n",
        "#from sklearn.datasets import make_classification\n",
        "# We generate data with 'Time' (0-172800 seconds) and 'Amount' features\n",
        "#N_SAMPLES = 5000\n",
        "#X, y = make_classification(n_samples=N_SAMPLES, n_features=28, n_informative=20,\n",
        "#                           weights=[0.99828, 0.00172], # 0.172% minority\n",
        "#                           random_state=42)\n",
        "\n",
        "# Create DataFrame to simulate real columns\n",
        "#cols = [f'V{i}' for i in range(1, 29)]\n",
        "#df_X = pd.DataFrame(X, columns=cols)\n",
        "# Add 'Time' (0 to 48 hours in seconds) and 'Amount' (with outliers)\n",
        "#df_X['Time'] = np.random.randint(0, 172800, size=N_SAMPLES)\n",
        "#df_X['Amount'] = np.random.exponential(scale=100, size=N_SAMPLES)\n",
        "#X = df_X # Use DataFrame for the pipeline\n",
        "X = df.drop('Class', axis=1)  # features\n",
        "y = df['Class'] # Labels\n",
        "\n",
        "print(f\"Dataset Shape: {X.shape}, Fraud Ratio: {np.mean(y):.4%}\")\n",
        "\n",
        "# --- 2. CUSTOM COMPONENTS ---\n",
        "\n",
        "class HourExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts 'Hour' from 'Time' feature to capture diurnal patterns.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        # Convert seconds to hour of day (0-23)\n",
        "        if isinstance(X_copy, pd.DataFrame):\n",
        "            X_copy['Hour'] = (X_copy['Time'] % (60*60*24)) // (60*60)\n",
        "            #return X_copy.drop(columns=['Time']) # Replace Time with Hour\n",
        "        return X_copy\n",
        "\n",
        "def pozzolo_correction(probs, prior_pos_orig, prior_pos_sampled):\n",
        "    \"\"\"\n",
        "    Applies Dal Pozzolo et al. (2015) calibration correction.\n",
        "    Formula: P_calib = (gamma * P_s) / (gamma * P_s + P_s_neg)\n",
        "    \"\"\"\n",
        "    if prior_pos_sampled == 0 or prior_pos_sampled == 1: return probs\n",
        "    gamma = (prior_pos_orig / (1 - prior_pos_orig)) / (prior_pos_sampled / (1 - prior_pos_sampled))\n",
        "    return (gamma * probs) / ((gamma * probs) + (1 - probs))\n",
        "\n",
        "# --- 3. ABLATION CONFIGURATIONS ---\n",
        "# We define distinct pipelines to test each component\n",
        "configs = {\n",
        "    \"1. Naive (No Preproc)\": {\n",
        "        'scale': False, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"2. + Scaling (Robust)\": {\n",
        "        'scale': True, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3. + Feature Eng (Hour)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"4. + RUS (Calibrated)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': False\n",
        "    },\n",
        "    \"5. + Optimization (Full)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. EXPERIMENTAL LOOP (Nested CV) ---\n",
        "# Outer Loop: Repeated Stratified 5-Fold\n",
        "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "# Inner Loop: Stratified 4-Fold (used inside RandomizedSearchCV)\n",
        "inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "results_table = []\n",
        "\n",
        "print(\"Starting Ablation Study (this may take a moment)...\")\n",
        "\n",
        "for name, cfg in configs.items():\n",
        "    print(f\"Running Configuration: {name}\")\n",
        "\n",
        "    fold_metrics = {'f2': [], 'f1': [], 'rec': [], 'prec': []}\n",
        "\n",
        "    # Outer CV loop (split training and test set)\n",
        "    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # --- A. Build Pipeline Steps ---\n",
        "        steps = []\n",
        "\n",
        "        # 1. Feature Engineering (Hour)\n",
        "        if cfg['fe']:\n",
        "            steps.append(('fe', HourExtractor()))\n",
        "        #else:\n",
        "            # Drop Time if not using FE (standard practice if raw Time is not useful)\n",
        "        #    steps.append(('drop_time', ColumnTransformer([('drop', 'drop', ['Time'])], remainder='passthrough')))\n",
        "\n",
        "        # 2. Scaling (RobustScaler)\n",
        "        if cfg['scale']:\n",
        "            # Apply robust scaler to Amount, pass through others\n",
        "            # @TODO\n",
        "            # Note: For simplicity in this demo, we apply to all numericals coming out of previous step\n",
        "            steps.append(('scaler', RobustScaler()))\n",
        "\n",
        "        # 3. Resampling (RUS)\n",
        "        # Resampling in the pipeline preventes data leakage\n",
        "        # Resampling is only applied to the traning fold inside\n",
        "        # (https://imbalanced-learn.org/stable/common_pitfalls.html)\n",
        "        if cfg['rus']:\n",
        "            steps.append(('rus', RandomUnderSampler(sampling_strategy=1.0, random_state=42)))\n",
        "\n",
        "        # 4. Classifier\n",
        "        # SciKit-Learn LogisticRegression\n",
        "        # (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "        # Regularization is applied by default\n",
        "        # Solver liblinear: supports L1 and L2 regularization\n",
        "        clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "        #clf = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
        "        #if cfg['rus']:\n",
        "             # If RUS is used, we don't need class_weight='balanced' usually,\n",
        "             # but keeping it doesn't hurt. strictly, RUS handles the balance.\n",
        "        #     clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "        steps.append(('clf', clf))\n",
        "\n",
        "        # Create pipeline from steps\n",
        "        pipeline = ImbPipeline(steps)\n",
        "\n",
        "        # --- B. Optimization (Inner Loop) ---\n",
        "        if cfg['opt']:\n",
        "            # Inner CV for optimisation\n",
        "            # (CV uses a fold of the train set for validation)\n",
        "            # Optimize for F2 Score\n",
        "\n",
        "            # Define distribution of the tuneable parameters\n",
        "            # print(clf.get_params()) # Print tunable parameters\n",
        "            # Naming convention of parameter names: stepname__parameter\n",
        "            # Here, the clf, the classifier of the pipeline is tuned\n",
        "            param_dist = {\n",
        "                'clf__C': loguniform(1e-4, 1e2) # Inverse of regularization strength\n",
        "            }\n",
        "            search = RandomizedSearchCV(pipeline, param_dist, n_iter=50,\n",
        "                                        scoring=make_scorer(fbeta_score, beta=2),\n",
        "                                        cv=inner_cv, n_jobs=-1, random_state=42)\n",
        "            search.fit(X_train, y_train)\n",
        "            model = search.best_estimator_\n",
        "        else:\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            model = pipeline\n",
        "\n",
        "        # --- C. Prediction & Calibration ---\n",
        "        # Get raw probabilities (biased if RUS was used)\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Apply Pozzolo Calibration ONLY if RUS was used\n",
        "        if cfg['rus']:\n",
        "            prior_pos_orig = np.mean(y_train == 1)\n",
        "            prior_pos_sampled = 0.5 # We forced sampling_strategy=1.0\n",
        "            probs = pozzolo_correction(probs, prior_pos_orig, prior_pos_sampled)\n",
        "\n",
        "        # Convert to Hard Predictions (Threshold = 0.5)\n",
        "        y_pred = (probs > 0.5).astype(int)\n",
        "\n",
        "        # --- D. Record Metrics ---\n",
        "        fold_metrics['f2'].append(fbeta_score(y_test, y_pred, beta=2))\n",
        "        fold_metrics['f1'].append(f1_score(y_test, y_pred))\n",
        "        fold_metrics['rec'].append(recall_score(y_test, y_pred))\n",
        "        fold_metrics['prec'].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    # Aggregate results for this configuration\n",
        "    results_table.append({\n",
        "        'Configuration': name,\n",
        "        'F2 Score (Mean ± SD)': f\"{np.mean(fold_metrics['f2']):.4f} ± {np.std(fold_metrics['f2']):.4f}\",\n",
        "        'F1 Score': f\"{np.mean(fold_metrics['f1']):.4f}\",\n",
        "        'Recall': f\"{np.mean(fold_metrics['rec']):.4f}\",\n",
        "        'Precision': f\"{np.mean(fold_metrics['prec']):.4f}\"\n",
        "    })\n",
        "\n",
        "# --- 5. OUTPUT ---\n",
        "df_results = pd.DataFrame(results_table)\n",
        "print(\"\\n=== Ablation Study Results (Baseline: Logistic Regression) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zj1FHRsaryp",
        "outputId": "0adff931-f312-4aea-82bd-a620c34d8ab1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (284807, 30), Fraud Ratio: 0.1727%\n",
            "Starting Ablation Study (this may take a moment)...\n",
            "Running Configuration: 1. Naive (No Preproc)\n",
            "Running Configuration: 2. + Scaling (Robust)\n",
            "Running Configuration: 3. + Feature Eng (Hour)\n",
            "Running Configuration: 4. + RUS (Calibrated)\n",
            "Running Configuration: 5. + Optimization (Full)\n",
            "\n",
            "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
            "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
            "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
            "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
            "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
            "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
            "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
            "| 5. + Optimization (Full) | 0.6052 ± 0.0515        |     0.6251 |   0.5946 |      0.6721 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06:\n",
        "\n",
        "\n",
        "```\n",
        "Dataset Shape: (284807, 30), Fraud Ratio: 0.1727%\n",
        "Starting Ablation Study (this may take a moment)...\n",
        "Running Configuration: 1. Naive (No Preproc)\n",
        "Running Configuration: 2. + Scaling (Robust)\n",
        "Running Configuration: 3. + Feature Eng (Hour)\n",
        "Running Configuration: 4. + RUS (Calibrated)\n",
        "Running Configuration: 5. + Optimization (Full)\n",
        "```\n",
        "\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.6052 ± 0.0515        |     0.6251 |   0.5946 |      0.6721 |"
      ],
      "metadata": {
        "id": "Vz0aY15t47E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "# This prints every single tunable parameter\n",
        "print(clf.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdmoNOrqGlch",
        "outputId": "b649f9bf-863c-446d-af6d-7d2141a8572a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(60*60*24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Ycwj_WKlUw",
        "outputId": "f8ae1217-5307-4d00-ec5f-2219c30c5337"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.best_params\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "F7rojAEzA7tR",
        "outputId": "170be772-1c68-4c6c-c93f-4fa7f4028df4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'search' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1949101497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
          ]
        }
      ]
    }
  ]
}