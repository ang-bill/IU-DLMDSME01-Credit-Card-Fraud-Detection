{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiPdke8S5yO4Zo11057r0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ang-bill/IU-DLMDSME01-Credit-Card-Fraud-Detection/blob/main/Task1_Credit_Card_Fraud_Detection_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Classifier 1"
      ],
      "metadata": {
        "id": "C9SieIYotZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A. Retrieve Dataset from Kaggle Hub\n",
        "At the first run, the dataset is downloaded from Kaggle and stored locally. Subsequent runs check whether the file already exists.\n",
        "See: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
      ],
      "metadata": {
        "id": "nWdKMlMnrOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28d_gYwkpQn",
        "outputId": "94ed39d1-0648-4ec0-c4b5-cf3f15d94561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'creditcard.csv' found locally at '/content/drive/MyDrive/Colab_Kaggle_Data/mlg-ulb/creditcardfraud/creditcard.csv'. Loading from there.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd # Pandas dataframe\n",
        "import kagglehub # Kagglehub to access dataset\n",
        "import shutil # Util for copying files\n",
        "from google.colab import drive # Import Google Drive utilities\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "local_storage_base_dir = \"/content/drive/MyDrive/Colab_Kaggle_Data\"\n",
        "\n",
        "# Dataset details\n",
        "kaggle_dataset_id = \"mlg-ulb/creditcardfraud\"\n",
        "file_name_in_dataset = \"creditcard.csv\"\n",
        "\n",
        "# Construct the full path to locally stored dataset\n",
        "local_dataset_dir = os.path.join(local_storage_base_dir, *kaggle_dataset_id.split('/'))\n",
        "full_local_file_path = os.path.join(local_dataset_dir, file_name_in_dataset)\n",
        "\n",
        "# Ensure the desired local storage directory exists\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "df = None # Initialize pandas df\n",
        "\n",
        "# Check if the file already exists in local storage, otherwise download from Kaggle\n",
        "if os.path.exists(full_local_file_path):\n",
        "    print(f\"'{file_name_in_dataset}' found locally at '{full_local_file_path}'. Loading from there.\")\n",
        "else:\n",
        "    print(f\"'{file_name_in_dataset}' not found locally. Attempting to download from KaggleHub and store it.\")\n",
        "\n",
        "    # Use kagglehub.dataset_download to get the dataset.\n",
        "    downloaded_source_root = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "\n",
        "    # Construct the path to the file within the KaggleHub download location\n",
        "    source_file_path = os.path.join(downloaded_source_root, file_name_in_dataset)\n",
        "\n",
        "    if os.path.exists(source_file_path):\n",
        "        print(f\"Dataset found at KaggleHub resolved location: '{source_file_path}'.\")\n",
        "        print(f\"Copying '{file_name_in_dataset}' to local path: '{full_local_file_path}'.\")\n",
        "\n",
        "        # Copy the file to local storage location\n",
        "        shutil.copy(source_file_path, full_local_file_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Failed to find '{file_name_in_dataset}' at source '{source_file_path}' after KaggleHub download resolution.\")\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv(full_local_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B. Implementation of Classifier 1\n"
      ],
      "metadata": {
        "id": "dulJshxnkvMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install dependencies\n",
        "(not included in default Colab Notebook)"
      ],
      "metadata": {
        "id": "LVQV_Xedpi9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uWLLZXfh48X7",
        "outputId": "de29782d-110b-443c-a296-97e5028911ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyod\n",
            "  Downloading pyod-2.0.6-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyod) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyod) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.12/dist-packages (from pyod) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.12/dist-packages (from pyod) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.6.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.17.0)\n",
            "Downloading pyod-2.0.6-py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyod\n",
            "Successfully installed pyod-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Customised Class on Feature Engineering\n",
        "\n",
        "\n",
        "*   HourExtractor: creates a new 'Hour' feature from 'Time'\n",
        "\n"
      ],
      "metadata": {
        "id": "JBnhr6oO0mtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HourExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts 'Hour' from 'Time' feature to capture diurnal patterns\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        # Convert seconds to hour of day (0-23)\n",
        "        if isinstance(X_copy, pd.DataFrame):\n",
        "            X_copy['Hour'] = (X_copy['Time'] % (60*60*24)) // (60*60)\n",
        "            #return X_copy.drop(columns=['Time']) # Replace Time with Hour\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "ktp61PnJ0DXx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Customised Class on Probability Calibration\n",
        "A customised classifier applies analytical probability calibration according to Dal Pozzolo et al. (2025). This approach enables integration with the scikit-learn library.\n",
        "\n",
        "https://doi.org/10.1109/SSCI.2015.33"
      ],
      "metadata": {
        "id": "EudJayPt3hnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import numpy as np\n",
        "\n",
        "class PozzoloCalibratedClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Wraps a classifier to apply Dal Pozzolo's prior correction automatically\n",
        "    during prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object\n",
        "        The base classifier (e.g., LogisticRegression or XGBClassifier).\n",
        "    original_prior : float\n",
        "        The prevalence of the positive class in the original dataset (e.g., 0.00172).\n",
        "    sampling_ratio : float, default=1.0\n",
        "        The target ratio used in RandomUnderSampler (1.0 means 50/50).\n",
        "    \"\"\"\n",
        "\n",
        "    _estimator_type = \"classifier\"\n",
        "\n",
        "    def __init__(self, estimator, original_prior=0.00172, sampling_ratio=1.0):\n",
        "        self.estimator = estimator\n",
        "        self.original_prior = original_prior\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Fit the internal model on the data provided (which is already RUS-sampled)\n",
        "        self.estimator.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        check_is_fitted(self.estimator)\n",
        "\n",
        "        # 1. Get Biased Probabilities (P_s) from the internal model\n",
        "        # The model thinks the world is 50% fraud because of RUS\n",
        "        probs_biased = self.estimator.predict_proba(X)\n",
        "\n",
        "        # If we only have 1 class in test (edge case), return as is\n",
        "        if probs_biased.shape[1] != 2:\n",
        "            return probs_biased\n",
        "\n",
        "        p_s = probs_biased[:, 1]\n",
        "\n",
        "        # 2. Calculate Correction Factor (Gamma)\n",
        "        # Gamma = (Original_Odds) / (Sampled_Odds)\n",
        "        # Sampled_Odds for ratio 1.0 is 0.5/0.5 = 1\n",
        "        prior_s = self.sampling_ratio / (1 + self.sampling_ratio) # e.g. 0.5\n",
        "\n",
        "        # Edge case protection\n",
        "        if self.original_prior <= 0 or self.original_prior >= 1:\n",
        "            return probs_biased\n",
        "\n",
        "        gamma = (self.original_prior / (1 - self.original_prior)) / \\\n",
        "                (prior_s / (1 - prior_s))\n",
        "\n",
        "        # 3. Apply Formula\n",
        "        p_calib = (gamma * p_s) / (gamma * p_s + (1 - p_s))\n",
        "\n",
        "        # Return in Scikit-Learn format [P(0), P(1)]\n",
        "        return np.vstack([1 - p_calib, p_calib]).T\n",
        "\n",
        "    def predict(self, X):\n",
        "        # This is the Key: predict() now uses the CALIBRATED probability\n",
        "        # So F2 Score optimization sees the real-world performance\n",
        "        probs = self.predict_proba(X)[:, 1]\n",
        "        return (probs > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Necessary for RandomizedSearchCV to access the inner 'estimator' params\n",
        "        params = super().get_params(deep)\n",
        "        if deep and hasattr(self.estimator, 'get_params'):\n",
        "            for key, value in self.estimator.get_params().items():\n",
        "                params[f'estimator__{key}'] = value\n",
        "        return params"
      ],
      "metadata": {
        "id": "lcOPOTgz40Op"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Required Packages"
      ],
      "metadata": {
        "id": "AS--3bgHqBCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, fbeta_score, f1_score, precision_score, recall_score, brier_score_loss\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Supports resampling inside CV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy.stats import loguniform"
      ],
      "metadata": {
        "id": "ciTsNFN8z7tI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Configuration of pipelines\n",
        "The ablation study evaluates the effect of each component separately, therefore distinct pipelines are defined.\n",
        "**Components:**\n",
        "*   Feature engineering: Augmentation with Hour vs. no feature engineering\n",
        "*   Data preprocessing: RobustScaler vs. no scaling\n",
        "*   Resampling: RUS vs. no resampling\n",
        "*   Classifier optimisation: Randomised parameter optimisation vs. using default parameters\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p41hJo3O6KkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define distinct pipelines to test each component\n",
        "configs = {\n",
        "    \"1. Naive (No Preproc)\": {\n",
        "        'scale': False, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"2. + Scaling (Robust)\": {\n",
        "        'scale': True, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3. + Feature Eng (Hour)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"4. + RUS (Calibrated)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': False\n",
        "    },\n",
        "    \"5. + Optimization (Full)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': True\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4efdI6Mp6Wtr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create features and labels"
      ],
      "metadata": {
        "id": "ELWDXcqNCKIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP: Synthetic Data mimicking your EDA findings ---\n",
        "#from sklearn.datasets import make_classification\n",
        "# We generate data with 'Time' (0-172800 seconds) and 'Amount' features\n",
        "#N_SAMPLES = 5000\n",
        "#X, y = make_classification(n_samples=N_SAMPLES, n_features=28, n_informative=20,\n",
        "#                           weights=[0.99828, 0.00172], # 0.172% minority\n",
        "#                           random_state=42)\n",
        "\n",
        "# Create DataFrame to simulate real columns\n",
        "#cols = [f'V{i}' for i in range(1, 29)]\n",
        "#df_X = pd.DataFrame(X, columns=cols)\n",
        "# Add 'Time' (0 to 48 hours in seconds) and 'Amount' (with outliers)\n",
        "#df_X['Time'] = np.random.randint(0, 172800, size=N_SAMPLES)\n",
        "#df_X['Amount'] = np.random.exponential(scale=100, size=N_SAMPLES)\n",
        "#X = df_X # Use DataFrame for the pipeline\n",
        "X = df.drop('Class', axis=1)  # features\n",
        "y = df['Class'] # Labels\n",
        "original_fraud_rate = np.mean(y)\n",
        "\n",
        "print(f\"Dataset Shape: {X.shape}, Fraud Ratio: {np.mean(y):.4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx5oGgFEHCL",
        "outputId": "ccb9fdae-9041-48c1-e352-63193ff7107a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (284807, 30), Fraud Ratio: 0.001727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Train, Validate, and Test"
      ],
      "metadata": {
        "id": "vGMQm6JvEYbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. EXPERIMENTAL LOOP (Nested CV) ---\n",
        "# Outer Loop: Repeated Stratified 5-Fold\n",
        "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "# Inner Loop: Stratified 4-Fold (used inside RandomizedSearchCV)\n",
        "inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "results_table = []\n",
        "\n",
        "print(\"Starting Ablation Study (this may take a moment)...\")\n",
        "\n",
        "for name, cfg in configs.items():\n",
        "    print(f\"Running Configuration: {name}\")\n",
        "\n",
        "    fold_metrics = {'f2': [], 'f1': [], 'rec': [], 'prec': []}\n",
        "\n",
        "    # Outer CV loop (split training and test set)\n",
        "    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # --- A. Build Pipeline Steps ---\n",
        "        steps = []\n",
        "\n",
        "        # 1. Feature Engineering (Hour)\n",
        "        if cfg['fe']:\n",
        "            steps.append(('fe', HourExtractor()))\n",
        "        #else:\n",
        "            # Drop Time if not using FE (standard practice if raw Time is not useful)\n",
        "        #    steps.append(('drop_time', ColumnTransformer([('drop', 'drop', ['Time'])], remainder='passthrough')))\n",
        "\n",
        "        # 2. Scaling (RobustScaler)\n",
        "        if cfg['scale']:\n",
        "            # Apply robust scaler to Amount, pass through others\n",
        "            # @TODO\n",
        "            # Note: For simplicity in this demo, we apply to all numericals coming out of previous step\n",
        "            steps.append(('scaler', RobustScaler()))\n",
        "\n",
        "        # 3. Resampling (RUS)\n",
        "        # Resampling in the pipeline preventes data leakage\n",
        "        # Resampling is only applied to the traning fold inside\n",
        "        # (https://imbalanced-learn.org/stable/common_pitfalls.html)\n",
        "        if cfg['rus']:\n",
        "            steps.append(('rus', RandomUnderSampler(sampling_strategy=1.0, random_state=42)))\n",
        "\n",
        "        # 4. Classifier\n",
        "        # SciKit-Learn LogisticRegression\n",
        "        # (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "        # Regularization is applied by default\n",
        "        # Solver liblinear: supports L1 and L2 regularization\n",
        "        #clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "        #clf = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
        "        if cfg['rus']:\n",
        "             # If RUS is used, we don't need class_weight='balanced' usually,\n",
        "             # but keeping it doesn't hurt. strictly, RUS handles the balance.\n",
        "        #     clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "              base_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "              final_clf = PozzoloCalibratedClassifier(\n",
        "                  estimator=base_clf,\n",
        "                  original_prior=original_fraud_rate,\n",
        "                  sampling_ratio=1.0\n",
        "              )\n",
        "        else:\n",
        "               final_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "        steps.append(('clf', final_clf))\n",
        "\n",
        "        # Create pipeline from steps\n",
        "        pipeline = ImbPipeline(steps)\n",
        "\n",
        "        # --- B. Optimization (Inner Loop) ---\n",
        "        if cfg['opt']:\n",
        "            # Inner CV for optimisation\n",
        "            # (CV uses a fold of the train set for validation)\n",
        "            # Optimize for F2 Score\n",
        "\n",
        "            # Define distribution of the tuneable parameters\n",
        "            # print(clf.get_params()) # Print tunable parameters\n",
        "            # Naming convention of parameter names: stepname__parameter\n",
        "            # Here, the clf, the classifier of the pipeline is tuned\n",
        "            clf_param_name = 'clf__estimator__C' if cfg['rus'] else 'clf__C'\n",
        "            param_dist = {\n",
        "                clf_param_name: loguniform(1e-4, 1e2) # Inverse of regularization strength\n",
        "            }\n",
        "            search = RandomizedSearchCV(pipeline, param_dist, n_iter=50,\n",
        "                                        scoring=make_scorer(fbeta_score, beta=2),\n",
        "                                        cv=inner_cv, n_jobs=-1, random_state=42)\n",
        "            search.fit(X_train, y_train)\n",
        "            model = search.best_estimator_\n",
        "        else:\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            model = pipeline\n",
        "\n",
        "        # --- C. Prediction & Calibration ---\n",
        "        # Get raw probabilities (biased if RUS was used)\n",
        "        #probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Convert to Hard Predictions (Threshold = 0.5)\n",
        "        #y_pred = (probs > 0.5).astype(int)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # --- D. Record Metrics ---\n",
        "        fold_metrics['f2'].append(fbeta_score(y_test, y_pred, beta=2))\n",
        "        fold_metrics['f1'].append(f1_score(y_test, y_pred))\n",
        "        fold_metrics['rec'].append(recall_score(y_test, y_pred))\n",
        "        fold_metrics['prec'].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "        fold_metrics['best_params'].append(search.best_params_)\n",
        "\n",
        "    # --- Aggregate results for this configuration ---\n",
        "\n",
        "    # Extract all C values found - key 'clf__estimator__C'\n",
        "    C_values_found = []\n",
        "    for params in fold_metrics['best_params']:\n",
        "        if 'clf__estimator__C' in params:\n",
        "            C_value = params['clf__estimator__C']\n",
        "            C_values_found.append(C_value)\n",
        "\n",
        "    # add to result table\n",
        "    results_table.append({\n",
        "        'Configuration': name,\n",
        "        'F2 Score (Mean)': f\"{np.mean(fold_metrics['f2']):.4f}\",\n",
        "        'F2 Score (SD)': f\"{np.std(fold_metrics['f2']):.4f}\",\n",
        "        'F1 Score (Mean)': f\"{np.mean(fold_metrics['f1']):.4f}\",\n",
        "        'F1 Score (SD)': f\"{np.std(fold_metrics['f1']):.4f}\",\n",
        "        'F2 Recall (Mean)': f\"{np.mean(fold_metrics['rec']):.4f}\",\n",
        "        'F2 Recall (SD)': f\"{np.std(fold_metrics['rec']):.4f}\",\n",
        "        'F1 Precision (Mean)': f\"{np.mean(fold_metrics['prec']):.4f}\",\n",
        "        'F1 Precision (SD)': f\"{np.std(fold_metrics['prec']):.4f}\",\n",
        "        'Best Params (Mean)': f\"{np.mean(C_values_found):.4f}\",\n",
        "        'Best Params (SD)': f\"{np.std(C_values_found):.4f}\",\n",
        "    })\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zj1FHRsaryp",
        "outputId": "069f48d1-9f05-4d32-fb85-6989bd3dc366"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ablation Study (this may take a moment)...\n",
            "Running Configuration: 1. Naive (No Preproc)\n",
            "Running Configuration: 2. + Scaling (Robust)\n",
            "Running Configuration: 3. + Feature Eng (Hour)\n",
            "Running Configuration: 4. + RUS (Calibrated)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Configuration: 5. + Optimization (Full)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py:65: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 0.15 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Output result"
      ],
      "metadata": {
        "id": "vgLHNGi2FOoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results_table)\n",
        "print(\"\\n=== Ablation Study Results (Baseline: Logistic Regression) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06WMszhEFWHZ",
        "outputId": "72ee6d7b-faa8-43b9-d634-b065390aa0d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
            "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
            "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
            "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
            "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
            "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
            "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
            "| 5. + Optimization (Full) | 0.7008 ± 0.0582        |     0.6215 |   0.774  |      0.5343 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 with PozzoloCalibratedClassifier :\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.7008 ± 0.0582        |     0.6215 |   0.774  |      0.5343 |\n"
      ],
      "metadata": {
        "id": "6AwBaeSVAYc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu4cufA0Dfar",
        "outputId": "75a530e0-9991-4a3b-843d-be8f3cb7905e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clf__estimator__C': np.float64(0.4042872735027334)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 without PozzoloCalibratedClassifier:\n",
        "\n",
        "\n",
        "```\n",
        "Dataset Shape: (284807, 30), Fraud Ratio: 0.1727%\n",
        "Starting Ablation Study (this may take a moment)...\n",
        "Running Configuration: 1. Naive (No Preproc)\n",
        "Running Configuration: 2. + Scaling (Robust)\n",
        "Running Configuration: 3. + Feature Eng (Hour)\n",
        "Running Configuration: 4. + RUS (Calibrated)\n",
        "Running Configuration: 5. + Optimization (Full)\n",
        "```\n",
        "\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean ± SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195 ± 0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569 ± 0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576 ± 0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978 ± 0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.6052 ± 0.0515        |     0.6251 |   0.5946 |      0.6721 |"
      ],
      "metadata": {
        "id": "Vz0aY15t47E9"
      }
    }
  ]
}