{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nWdKMlMnrOi-",
        "LVQV_Xedpi9F",
        "YJG9XyPEEduk",
        "iwgB_kvnEoBh",
        "KtqPptVwEsVc",
        "myISoy9oEu2s",
        "EudJayPt3hnz",
        "T9Bh0H5AnKO1"
      ],
      "toc_visible": true,
      "mount_file_id": "1z4jM6F-HazoVJXShIOLUPMaGK-oW5awt",
      "authorship_tag": "ABX9TyPfBru4ytIdZdJKhPuNhaBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ang-bill/IU-DLMDSME01-Credit-Card-Fraud-Detection/blob/main/Task1_Credit_Card_Fraud_Detection_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Classifier 1"
      ],
      "metadata": {
        "id": "C9SieIYotZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A. Retrieve Dataset from Kaggle Hub\n",
        "At the first run, the dataset is downloaded from Kaggle and stored locally. Subsequent runs check whether the file already exists.\n",
        "See: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
      ],
      "metadata": {
        "id": "nWdKMlMnrOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28d_gYwkpQn",
        "outputId": "538fdcac-5dbe-47d8-eb57-562b352b85a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'creditcard.csv' found locally at '/content/drive/MyDrive/Colab_Kaggle_Data/mlg-ulb/creditcardfraud/creditcard.csv'. Loading from there.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd # Pandas dataframe\n",
        "import kagglehub # Kagglehub to access dataset\n",
        "import shutil # Util for copying files\n",
        "from google.colab import drive # Import Google Drive utilities\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "local_storage_base_dir = \"/content/drive/MyDrive/Colab_Kaggle_Data\"\n",
        "\n",
        "# Dataset details\n",
        "kaggle_dataset_id = \"mlg-ulb/creditcardfraud\"\n",
        "file_name_in_dataset = \"creditcard.csv\"\n",
        "\n",
        "# Construct the full path to locally stored dataset\n",
        "local_dataset_dir = os.path.join(local_storage_base_dir, *kaggle_dataset_id.split('/'))\n",
        "full_local_file_path = os.path.join(local_dataset_dir, file_name_in_dataset)\n",
        "\n",
        "# Ensure the desired local storage directory exists\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "df = None # Initialize pandas df\n",
        "\n",
        "# Check if the file already exists in local storage, otherwise download from Kaggle\n",
        "if os.path.exists(full_local_file_path):\n",
        "    print(f\"'{file_name_in_dataset}' found locally at '{full_local_file_path}'. Loading from there.\")\n",
        "else:\n",
        "    print(f\"'{file_name_in_dataset}' not found locally. Attempting to download from KaggleHub and store it.\")\n",
        "\n",
        "    # Use kagglehub.dataset_download to get the dataset.\n",
        "    downloaded_source_root = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "\n",
        "    # Construct the path to the file within the KaggleHub download location\n",
        "    source_file_path = os.path.join(downloaded_source_root, file_name_in_dataset)\n",
        "\n",
        "    if os.path.exists(source_file_path):\n",
        "        print(f\"Dataset found at KaggleHub resolved location: '{source_file_path}'.\")\n",
        "        print(f\"Copying '{file_name_in_dataset}' to local path: '{full_local_file_path}'.\")\n",
        "\n",
        "        # Copy the file to local storage location\n",
        "        shutil.copy(source_file_path, full_local_file_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Failed to find '{file_name_in_dataset}' at source '{source_file_path}' after KaggleHub download resolution.\")\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv(full_local_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B. Implementation of Classifier - Base\n"
      ],
      "metadata": {
        "id": "dulJshxnkvMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install dependencies\n",
        "(not included in default Colab Notebook)"
      ],
      "metadata": {
        "id": "LVQV_Xedpi9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uWLLZXfh48X7",
        "outputId": "0e5cbc01-89f1-4101-a5bb-7b91ee9f94dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyod in /usr/local/lib/python3.12/dist-packages (2.0.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyod) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyod) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.12/dist-packages (from pyod) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.12/dist-packages (from pyod) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from pyod) (1.6.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Customised Class on Feature Engineering"
      ],
      "metadata": {
        "id": "JBnhr6oO0mtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HourExtractor"
      ],
      "metadata": {
        "id": "YJG9XyPEEduk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HourExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts 'Hour' from 'Time' feature to capture diurnal patterns\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Convert seconds to hour of day (0-23)\n",
        "        X_copy['Hour'] = (X_copy['Time'] % (60*60*24)) // (60*60)\n",
        "\n",
        "        #return X_copy.drop(columns=['Time']) # Replace Time with Hour\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "ktp61PnJ0DXx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DayNightExtractor"
      ],
      "metadata": {
        "id": "iwgB_kvnEoBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DayNightExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts 'Is_Night' from 'Time' feature to capture diurnal patterns\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Convert seconds to hour of day (0-23)\n",
        "        #X_copy['Hour'] = (X_copy['Time'] % (60*60*24)) // (60*60)\n",
        "\n",
        "        # Is_Night (Binary): 1 if between 24:00 and 08:00 (convert to hours first)\n",
        "        X_copy['Is_Night'] = (((X_copy['Time'] % (60*60*24)) // (60*60)) <= 8)\n",
        "        X_copy['Is_Night'] = X_copy['Is_Night'].astype(int)\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "_Dw0Lv0Kv_eY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LogAmountExtractor"
      ],
      "metadata": {
        "id": "KtqPptVwEsVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class LogAmountExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts Log Amount from Amount to handle the extreme skew\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Log Amount: Handles the extreme skew of transaction amounts\n",
        "        # np.log1p avoids log(0) errors\n",
        "        X_copy['Log_Amount'] = np.log1p(X_copy['Amount'])\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "ufPdSKF3DYSb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V14V17InteractionExtractor"
      ],
      "metadata": {
        "id": "myISoy9oEu2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class V14V17InteractionExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts a combined feature from V14 and V17 to capture diagonal patterns\n",
        "    and creates a new feature.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Log Amount: Handles the extreme skew of transaction amounts\n",
        "        # np.log1p avoids log(0) errors\n",
        "        X_copy['V14_V17'] = X_copy['V14'] * X_copy['V17']\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "Sw9Q_yGmD_Dh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Customised Class on Probability Calibration\n",
        "A customised classifier applies analytical probability calibration according to Dal Pozzolo et al. (2025). This approach enables integration with the scikit-learn library.\n",
        "\n",
        "https://doi.org/10.1109/SSCI.2015.33"
      ],
      "metadata": {
        "id": "EudJayPt3hnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import numpy as np\n",
        "\n",
        "class PozzoloCalibratedClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Wraps a classifier to apply Dal Pozzolo's prior correction automatically\n",
        "    during prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object\n",
        "        The base classifier (e.g., LogisticRegression or XGBClassifier).\n",
        "    original_prior : float\n",
        "        The prevalence of the positive class in the original dataset (e.g., 0.00172).\n",
        "    sampling_ratio : float, default=1.0\n",
        "        The target ratio used in RandomUnderSampler (1.0 means 50/50).\n",
        "    \"\"\"\n",
        "\n",
        "    _estimator_type = \"classifier\"\n",
        "\n",
        "    def __init__(self, estimator, original_prior=0.00172, sampling_ratio=1.0):\n",
        "        self.estimator = estimator\n",
        "        self.original_prior = original_prior\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Fit the internal model on the data provided (which is already RUS-sampled)\n",
        "        self.estimator.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        check_is_fitted(self.estimator)\n",
        "\n",
        "        # 1. Get Biased Probabilities (P_s) from the internal model\n",
        "        # The model thinks the world is 50% fraud because of RUS\n",
        "        probs_biased = self.estimator.predict_proba(X)\n",
        "\n",
        "        # If we only have 1 class in test (edge case), return as is\n",
        "        if probs_biased.shape[1] != 2:\n",
        "            return probs_biased\n",
        "\n",
        "        p_s = probs_biased[:, 1]\n",
        "\n",
        "        # 2. Calculate Correction Factor (Gamma)\n",
        "        # Gamma = (Original_Odds) / (Sampled_Odds)\n",
        "        # Sampled_Odds for ratio 1.0 is 0.5/0.5 = 1\n",
        "        prior_s = self.sampling_ratio / (1 + self.sampling_ratio) # e.g. 0.5\n",
        "\n",
        "        # Edge case protection\n",
        "        if self.original_prior <= 0 or self.original_prior >= 1:\n",
        "            return probs_biased\n",
        "\n",
        "        gamma = (self.original_prior / (1 - self.original_prior)) / \\\n",
        "                (prior_s / (1 - prior_s))\n",
        "\n",
        "        # 3. Apply Formula\n",
        "        p_calib = (gamma * p_s) / (gamma * p_s + (1 - p_s))\n",
        "\n",
        "        # Return in Scikit-Learn format [P(0), P(1)]\n",
        "        return np.vstack([1 - p_calib, p_calib]).T\n",
        "\n",
        "    def predict(self, X):\n",
        "        # This is the Key: predict() now uses the CALIBRATED probability\n",
        "        # So F2 Score optimization sees the real-world performance\n",
        "        probs = self.predict_proba(X)[:, 1]\n",
        "        return (probs > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Necessary for RandomizedSearchCV to access the inner 'estimator' params\n",
        "        params = super().get_params(deep)\n",
        "        if deep and hasattr(self.estimator, 'get_params'):\n",
        "            for key, value in self.estimator.get_params().items():\n",
        "                params[f'estimator__{key}'] = value\n",
        "        return params"
      ],
      "metadata": {
        "id": "lcOPOTgz40Op"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Customised Class TOS Generator for XGBOD"
      ],
      "metadata": {
        "id": "T9Bh0H5AnKO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TOSGenerator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Generates Transformed Outlier Scores (TOS) for XGBOD.\n",
        "    Wraps PyOD unsupervised detectors to act as feature extractors.\n",
        "    \"\"\"\n",
        "    def __init__(self, detectors=None):\n",
        "        self.detectors = detectors\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # If no detectors provided, initialize default ones\n",
        "        # Note: We initialize here to ensure fresh models for every fold\n",
        "        if self.detectors is None:\n",
        "            self.detectors = [KNN(n_neighbors=5), LOF(n_neighbors=5)]\n",
        "\n",
        "        for detector in self.detectors:\n",
        "            detector.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        tos_features = []\n",
        "        for detector in self.detectors:\n",
        "            # For PyOD, decision_function returns the anomaly score\n",
        "            # Reshape to (n_samples, 1)\n",
        "            scores = detector.decision_function(X).reshape(-1, 1)\n",
        "            tos_features.append(scores)\n",
        "\n",
        "        # Stack original features with new TOS features\n",
        "        return np.hstack([X] + tos_features)"
      ],
      "metadata": {
        "id": "eoxg-9INnDfM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Required Packages"
      ],
      "metadata": {
        "id": "AS--3bgHqBCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.base import clone, BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, fbeta_score, f1_score, precision_score, recall_score, brier_score_loss\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Supports resampling inside CV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "import warnings\n",
        "# Filter specific warning from imblearn pipeline (supress as pipeline works correctly)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"imblearn.pipeline\")"
      ],
      "metadata": {
        "id": "ciTsNFN8z7tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Configuration of pipelines\n",
        "The ablation study evaluates the effect of each component separately, therefore distinct pipelines are defined.\n",
        "\n",
        "**Components:**\n",
        "*   Feature engineering: Augmentation with Hour vs. no feature engineering\n",
        "*   Data preprocessing: RobustScaler vs. no scaling\n",
        "*   Resampling: RUS vs. no resampling\n",
        "*   Classifier optimisation: Randomised parameter optimisation vs. using default parameters\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p41hJo3O6KkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define distinct pipelines to test each component\n",
        "configs = {\n",
        "    \"1. Naive (No Preproc)\": {\n",
        "        'scale': False, 'fe_1': False, 'fe_2': False, 'fe_3': False, 'fe_4': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"2. + Scaling (Robust)\": {\n",
        "        'scale': True, 'fe_1': False, 'fe_2': False, 'fe_3': False, 'fe_4': False,  'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3.A + Feature Eng (Hour)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': False, 'fe_3': False, 'fe_4': False,  'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3.A + Feature Eng (DayNight)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': False, 'fe_4': False,  'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3.A + Feature Eng (LogAmount)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': True, 'fe_4': False,  'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3.A + Feature Eng (V14-V17-Interaction)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': True, 'fe_4': True,  'rus': False, 'opt': False\n",
        "    },\n",
        "    \"4. + RUS (Calibrated)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': True, 'fe_4': True,  'rus': True, 'opt': False\n",
        "    },\n",
        "    \"5. + Optimization (with RUS)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': True, 'fe_4': True,  'rus': True, 'opt': True\n",
        "    },\n",
        "    \"6. + Optimization (without RUS)\": {\n",
        "        'scale': True, 'fe_1': True, 'fe_2': True, 'fe_3': True, 'fe_4': True,  'rus': False, 'opt': True\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4efdI6Mp6Wtr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create features and labels"
      ],
      "metadata": {
        "id": "ELWDXcqNCKIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP: Synthetic Data mimicking your EDA findings ---\n",
        "#from sklearn.datasets import make_classification\n",
        "# We generate data with 'Time' (0-172800 seconds) and 'Amount' features\n",
        "#N_SAMPLES = 5000\n",
        "#X, y = make_classification(n_samples=N_SAMPLES, n_features=28, n_informative=20,\n",
        "#                           weights=[0.99828, 0.00172], # 0.172% minority\n",
        "#                           random_state=42)\n",
        "\n",
        "# Create DataFrame to simulate real columns\n",
        "#cols = [f'V{i}' for i in range(1, 29)]\n",
        "#df_X = pd.DataFrame(X, columns=cols)\n",
        "# Add 'Time' (0 to 48 hours in seconds) and 'Amount' (with outliers)\n",
        "#df_X['Time'] = np.random.randint(0, 172800, size=N_SAMPLES)\n",
        "#df_X['Amount'] = np.random.exponential(scale=100, size=N_SAMPLES)\n",
        "#X = df_X # Use DataFrame for the pipeline\n",
        "X = df.drop('Class', axis=1)  # features\n",
        "y = df['Class'] # Labels\n",
        "original_fraud_rate = np.mean(y)\n",
        "\n",
        "print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Dataset Shape: {X.shape}, Fraud Ratio: {np.mean(y):.4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx5oGgFEHCL",
        "outputId": "8537e9e2-6d8a-4da2-bd0b-d4b36a583271"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-11 13:02:09 Dataset Shape: (284807, 30), Fraud Ratio: 0.001727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Run Ablation Study (Train, Validate, and Test)"
      ],
      "metadata": {
        "id": "vGMQm6JvEYbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ablation_study(experiment_name, base_clf, param_dist, X, y, configs, extra_pipeline_steps=None):\n",
        "    \"\"\"\n",
        "    Runs the full ablation study for a specific classifier.\n",
        "\n",
        "    Parameters:\n",
        "    - experiment_name: String label (e.g., \"XGBoost\")\n",
        "    - base_clf: The instantiated classifier object (e.g., XGBClassifier())\n",
        "    - param_dist: Dictionary for RandomizedSearchCV\n",
        "    - X, y: Data\n",
        "    - configs: Dictionary of ablation configurations\n",
        "    - extra_pipeline_steps: List of (name, transformer) tuples to add before scaling (e.g., TOS)\n",
        "    \"\"\"\n",
        "\n",
        "    # Cross Validation Outer Loop: Repeated Stratified 5-Fold\n",
        "    #outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
        "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "    # Cross Validation Inner Loop: Stratified 4-Fold (used inside RandomizedSearchCV)\n",
        "    inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    print(f\"--- Starting Experiment: {experiment_name} ---\")\n",
        "\n",
        "    for name, cfg in configs.items():\n",
        "        print(f\"\\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Running Configuration: {name}\")\n",
        "\n",
        "        fold_metrics = {\n",
        "                'f2_test': [], 'f2_train': [],\n",
        "                'f1_test': [], 'f1_train': [],\n",
        "                'rec_test': [], 'rec_train': [],\n",
        "                'prec_test': [], 'prec_train': [],\n",
        "                'best_params': []\n",
        "            }\n",
        "\n",
        "        # Outer CV loop (split training and test set)\n",
        "        for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # --- A. Build Pipeline Steps ---\n",
        "            steps = []\n",
        "\n",
        "            # 1. Feature Engineering\n",
        "            #if cfg['fe']:\n",
        "                #steps.append(('fe', HourExtractor()))\n",
        "                #steps.append(('fe', FraudFeatureEngineer()))\n",
        "            if cfg['fe_1']:\n",
        "                steps.append(('fe_1', HourExtractor()))\n",
        "            if cfg['fe_2']:\n",
        "                steps.append(('fe_2', DayNightExtractor()))\n",
        "            if cfg['fe_3']:\n",
        "                steps.append(('fe_3', LogAmountExtractor()))\n",
        "            if cfg['fe_4']:\n",
        "                steps.append(('fe_4', V14V17InteractionExtractor()))\n",
        "\n",
        "\n",
        "            # 1b. Extra Steps (e.g., XGBOD TOS Generator)\n",
        "            if extra_pipeline_steps:\n",
        "                steps.extend(extra_pipeline_steps)\n",
        "\n",
        "            # 2. Scaling (RobustScaler)\n",
        "            if cfg['scale']:\n",
        "                # Apply robust scaler to Amount, pass through others\n",
        "                # @TODO\n",
        "                # Note: For simplicity in this demo, we apply to all numericals coming out of previous step\n",
        "                steps.append(('scaler', RobustScaler()))\n",
        "\n",
        "            # 3. Resampling (RUS)\n",
        "            # Resampling in the pipeline preventes data leakage\n",
        "            # Resampling is only applied to the traning fold inside\n",
        "            # (https://imbalanced-learn.org/stable/common_pitfalls.html)\n",
        "            if cfg['rus']:\n",
        "                steps.append(('rus', RandomUnderSampler(sampling_strategy=1.0, random_state=42)))\n",
        "\n",
        "            # 4. Classifier\n",
        "            # SciKit-Learn LogisticRegression\n",
        "            # (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "            # Regularization is applied by default\n",
        "            # Solver liblinear: supports L1 and L2 regularization\n",
        "            if cfg['rus']:\n",
        "                  #base_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "                  #final_clf = PozzoloCalibratedClassifier(\n",
        "                  #    estimator=base_clf,\n",
        "                  #    original_prior=original_fraud_rate,\n",
        "                  #    sampling_ratio=1.0\n",
        "                  #)\n",
        "                  # Wrap with Pozzolo Calibration\n",
        "                  final_clf = PozzoloCalibratedClassifier(\n",
        "                      estimator=clone(base_clf), # clone base_clf to ensure fresh start\n",
        "                      original_prior=original_fraud_rate,\n",
        "                      sampling_ratio=1.0\n",
        "                  )\n",
        "            else:\n",
        "                  #final_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "                  final_clf = clone(base_clf)\n",
        "\n",
        "            steps.append(('clf', final_clf))\n",
        "\n",
        "            # Create pipeline from steps\n",
        "            pipeline = ImbPipeline(steps)\n",
        "\n",
        "            # --- B. Optimization (Inner Loop) ---\n",
        "            if cfg['opt']:\n",
        "                # Inner CV for optimisation\n",
        "                # (CV uses a fold of the train set for validation)\n",
        "                # Optimize for F2 Score\n",
        "\n",
        "                # Define distribution of the tuneable parameters\n",
        "                # print(clf.get_params()) # Print tunable parameters\n",
        "                # Naming convention of parameter names: stepname__parameter\n",
        "                # Here, the clf, the classifier of the pipeline is tuned\n",
        "                #clf_param_name = 'clf__estimator__C' if cfg['rus'] else 'clf__C'\n",
        "                #param_dist = {\n",
        "                #    clf_param_name: loguniform(1e-4, 1e2) # Inverse of regularization strength\n",
        "                #}\n",
        "\n",
        "                iterations = 50 if cfg['rus'] else 15\n",
        "\n",
        "                # Adjust param names for wrapper vs standalone\n",
        "                # If wrapped: clf__estimator__param\n",
        "                # If standalone: clf__param\n",
        "                prefix = 'clf__estimator__' if cfg['rus'] else 'clf__'\n",
        "\n",
        "                # Update param_dist keys to match current pipeline structure\n",
        "                tuned_params = {f\"{prefix}{k}\": v for k, v in param_dist.items()}\n",
        "\n",
        "                search = RandomizedSearchCV(pipeline, tuned_params,\n",
        "                                            n_iter=iterations,\n",
        "                                            scoring=make_scorer(fbeta_score, beta=2),\n",
        "                                            cv=inner_cv, n_jobs=-1,\n",
        "                                            random_state=42)\n",
        "                search.fit(X_train, y_train)\n",
        "                model = search.best_estimator_\n",
        "            else:\n",
        "                pipeline.fit(X_train, y_train)\n",
        "                model = pipeline\n",
        "\n",
        "            # Prediction on test set\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Prediction also on training set\n",
        "            # This enables the measurement of the generalization gap between\n",
        "            # the training and the test accuracy to control overfitting. See\n",
        "            # https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
        "            # Test metrics: measures performance on test data (never seen by model)\n",
        "            # Training metrics: measures performance on training data (all inner\n",
        "            #                   folds of CV)\n",
        "            y_pred_train = model.predict(X_train)\n",
        "\n",
        "            # Record Metrics on test set\n",
        "            fold_metrics['f2_test'].append(fbeta_score(y_test, y_pred, beta=2))\n",
        "            fold_metrics['f1_test'].append(f1_score(y_test, y_pred))\n",
        "            fold_metrics['rec_test'].append(recall_score(y_test, y_pred))\n",
        "            fold_metrics['prec_test'].append(precision_score(y_test, y_pred, zero_division=0))\n",
        "\n",
        "            # Record Metrics on training set\n",
        "            fold_metrics['f2_train'].append(fbeta_score(y_train, y_pred_train, beta=2))\n",
        "            fold_metrics['f1_train'].append(f1_score(y_train, y_pred_train))\n",
        "            fold_metrics['rec_train'].append(recall_score(y_train, y_pred_train))\n",
        "            fold_metrics['prec_train'].append(precision_score(y_train, y_pred_train, zero_division=0))\n",
        "\n",
        "            # Record best optimization parameters\n",
        "            if cfg['opt']and len(fold_metrics['best_params']) > 0 :\n",
        "              fold_metrics['best_params'].append(search.best_params_)\n",
        "              #params_df = pd.DataFrame(search.best_params_)\n",
        "              #print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "              print(pd.DataFrame(search.best_params_).to_markdown(index=False))\n",
        "\n",
        "        # --- Aggregate results for this configuration ---\n",
        "        row = {'Configuration': name}\n",
        "        metrics_map = {\n",
        "          'f2': 'F2 Score',\n",
        "          'f1': 'F1 Score',\n",
        "          'rec': 'Recall',\n",
        "          'prec': 'Precision'\n",
        "          }\n",
        "\n",
        "        for key, label in metrics_map.items():\n",
        "          # 1. Get Lists\n",
        "          test_scores = fold_metrics[f'{key}_test']\n",
        "          train_scores = fold_metrics[f'{key}_train']\n",
        "\n",
        "          # 2. Calculate Stats\n",
        "          test_mean = np.mean(test_scores)\n",
        "          test_std  = np.std(test_scores)\n",
        "          train_mean = np.mean(train_scores)\n",
        "\n",
        "          # 3. Calculate Gap (Train - Test)\n",
        "          gap = train_mean - test_mean\n",
        "\n",
        "          # 4. Calculate Percentages (Safe Division)\n",
        "          if test_mean > 0:\n",
        "              cv_pct = (test_std / test_mean) * 100       # Coefficient of Variation\n",
        "              gap_pct = (gap / test_mean) * 100           # Relative Overfitting\n",
        "          else:\n",
        "              cv_pct = 0.0\n",
        "              gap_pct = 0.0\n",
        "\n",
        "          # 5. Add Columns to Row\n",
        "          row[f'{label} (Mean)'] = f\"{test_mean:.4f}\"\n",
        "          row[f'{label} (SD)']   = f\"{test_std:.4f} ({cv_pct:.1f}%)\"\n",
        "          row[f'{label} (Gap)']  = f\"{gap:.4f} ({gap_pct:.1f}%)\"\n",
        "\n",
        "        # Add hyperparameters\n",
        "\n",
        "        # Extract all C values found - key 'clf__estimator__C'\n",
        "        #C_values_found = []\n",
        "        #for params in fold_metrics['best_params']:\n",
        "        #    if 'clf__estimator__C' in params:\n",
        "        #        C_value = params['clf__estimator__C']\n",
        "        #        C_values_found.append(C_value)\n",
        "\n",
        "        # Calculate mean and sd for hyperparameters\n",
        "        params_df = pd.DataFrame(fold_metrics['best_params'])\n",
        "        numeric_params_df = params_df.select_dtypes(include=[np.number])\n",
        "        mean_params = numeric_params_df.mean()\n",
        "        sd_params = numeric_params_df.std(ddof=1) # Use ddof=1 for sample SD\n",
        "\n",
        "        for param_name in mean_params.index:\n",
        "            mean_val = mean_params[param_name]\n",
        "            sd_val = sd_params[param_name]\n",
        "\n",
        "            # Use a clean, generic column name (e.g., C_Mean)\n",
        "            base_name = param_name.split('__')[-1] # Extracts 'C' from 'clf__C'\n",
        "\n",
        "            row[f'{base_name} (Mean)'] = f\"{mean_val:.4f}\"\n",
        "            row[f'{base_name} (SD)'] = f\"{sd_val:.4f}\"\n",
        "\n",
        "        # 5. Append the complete row to the results table\n",
        "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        print(pd.DataFrame([row]).to_markdown(index=False))\n",
        "        results_table.append(row)\n",
        "\n",
        "    # --- Return result ---\n",
        "    return pd.DataFrame(results_table)"
      ],
      "metadata": {
        "id": "8Zj1FHRsaryp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Precision - Recall - AUC"
      ],
      "metadata": {
        "id": "guf9aAyohIqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pr_curves_with_std(plotting_data, title=\"Precision-Recall Curve Comparison\"):\n",
        "    \"\"\"\n",
        "    Plots the Mean Precision-Recall Curve with Standard Deviation shading.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Iterate over each configuration (e.g., \"Naive\", \"XGBoost + RUS\", etc.)\n",
        "    for name, data in plotting_data.items():\n",
        "        y_real_folds = data['y_real']\n",
        "        y_proba_folds = data['y_proba']\n",
        "\n",
        "        # We will interpolate precision at these fixed recall points\n",
        "        mean_recall = np.linspace(0, 1, 100)\n",
        "        precisions = []\n",
        "\n",
        "        # Process every fold\n",
        "        for y_true, y_scores in zip(y_real_folds, y_proba_folds):\n",
        "            # Calculate PR curve for this fold\n",
        "            precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "            # Interpolate precision to match the fixed mean_recall grid\n",
        "            # Note: PR curve is usually plotted Recall (x) vs Precision (y)\n",
        "            # We reverse arguments for interp because recall is decreasing in the raw output\n",
        "            interp_precision = np.interp(mean_recall, recall[::-1], precision[::-1])\n",
        "            precisions.append(interp_precision)\n",
        "\n",
        "        # Convert to array for stats\n",
        "        precisions = np.array(precisions)\n",
        "\n",
        "        # Calculate Mean and SD\n",
        "        mean_precision = np.mean(precisions, axis=0)\n",
        "        std_precision = np.std(precisions, axis=0)\n",
        "\n",
        "        # Calculate Mean AUC (Area Under Curve) - Average Precision\n",
        "        mean_auc = auc(mean_recall, mean_precision)\n",
        "\n",
        "        # --- PLOTTING ---\n",
        "        p = plt.plot(mean_recall, mean_precision,\n",
        "                     label=f\"{name} (AUC = {mean_auc:.3f})\",\n",
        "                     linewidth=2, alpha=0.9)\n",
        "\n",
        "        # Add Shading for Standard Deviation (Robustness)\n",
        "        plt.fill_between(mean_recall,\n",
        "                         np.maximum(mean_precision - std_precision, 0),\n",
        "                         np.minimum(mean_precision + std_precision, 1),\n",
        "                         color=p[0].get_color(), alpha=0.15)\n",
        "\n",
        "    # Decoration\n",
        "    plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "    plt.ylabel('Precision (Positive Predictive Value)', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"upper right\", fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.02])\n",
        "\n",
        "    # Add baseline (No Skill)\n",
        "    # Fraud Ratio ~ 0.0017\n",
        "    no_skill = 0.00172\n",
        "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='gray', label='No Skill (Baseline)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# --- HOW TO RUN ---\n",
        "# 1. Run the study and get the data\n",
        "# df_results, plot_data = run_ablation_study_with_plot(..., configs=configs)\n",
        "#\n",
        "# 2. Plot\n",
        "# plot_pr_curves_with_std(plot_data, title=\"Impact of Feature Engineering on XGBoost Performance\")"
      ],
      "metadata": {
        "id": "bmBVbwwehXYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2C. Implementation of Base Classifier - Logistic Regression\n"
      ],
      "metadata": {
        "id": "T6jH03SJhQ1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "\n",
        "lr_params = {'C': loguniform(1e-4, 1e2)}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"Logistic Regression\",\n",
        "    base_clf=LogisticRegression(solver='liblinear', random_state=42),\n",
        "    param_dist=lr_params,\n",
        "    X=X, y=y, configs=configs\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: Logistic Regression) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLBnB6phmEP",
        "outputId": "f3571d09-1474-4935-cb6a-a6e609ed98d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Experiment: Logistic Regression ---\n",
            "\n",
            "2025-12-11 13:04:44 Running Configuration: 1. Naive (No Preproc)\n",
            "2025-12-11 13:05:10\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)    | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:----------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:---------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 1. Naive (No Preproc) |            0.6101 | 0.0708 (11.6%)  | 0.0217 (3.6%)    |            0.6614 | 0.0504 (7.6%)   | 0.0215 (3.2%)    |          0.5813 | 0.0815 (14.0%) | 0.0208 (3.6%)  |             0.7806 | 0.0450 (5.8%)    | 0.0115 (1.5%)     |\n",
            "\n",
            "2025-12-11 13:05:10 Running Configuration: 2. + Scaling (Robust)\n",
            "2025-12-11 13:05:47\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:----------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 2. + Scaling (Robust) |            0.6607 | 0.0378 (5.7%)   | 0.0060 (0.9%)    |             0.729 | 0.0324 (4.5%)   | 0.0056 (0.8%)    |          0.6221 | 0.0408 (6.6%) | 0.0060 (1.0%)  |             0.8827 | 0.0329 (3.7%)    | 0.0021 (0.2%)     |\n",
            "\n",
            "2025-12-11 13:05:47 Running Configuration: 3.A + Feature Eng (Hour)\n",
            "2025-12-11 13:06:20\n",
            "\n",
            "| Configuration            |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:-------------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 3.A + Feature Eng (Hour) |             0.657 | 0.0329 (5.0%)   | 0.0098 (1.5%)    |            0.7261 | 0.0292 (4.0%)   | 0.0087 (1.2%)    |           0.618 | 0.0351 (5.7%) | 0.0101 (1.6%)  |             0.8821 | 0.0329 (3.7%)    | 0.0033 (0.4%)     |\n",
            "\n",
            "2025-12-11 13:06:20 Running Configuration: 3.A + Feature Eng (DayNight)\n",
            "2025-12-11 13:06:58\n",
            "\n",
            "| Configuration                |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:-----------------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 3.A + Feature Eng (DayNight) |            0.6608 | 0.0326 (4.9%)   | 0.0105 (1.6%)    |            0.7292 | 0.0287 (3.9%)   | 0.0089 (1.2%)    |           0.622 | 0.0351 (5.6%) | 0.0111 (1.8%)  |             0.8828 | 0.0328 (3.7%)    | 0.0022 (0.2%)     |\n",
            "\n",
            "2025-12-11 13:06:58 Running Configuration: 3.A + Feature Eng (LogAmount)\n",
            "2025-12-11 13:07:32\n",
            "\n",
            "| Configuration                 |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:------------------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 3.A + Feature Eng (LogAmount) |            0.6608 | 0.0316 (4.8%)   | 0.0100 (1.5%)    |            0.7292 | 0.0286 (3.9%)   | 0.0083 (1.1%)    |          0.6221 | 0.0336 (5.4%) | 0.0106 (1.7%)  |             0.8827 | 0.0334 (3.8%)    | 0.0015 (0.2%)     |\n",
            "\n",
            "2025-12-11 13:07:32 Running Configuration: 3.A + Feature Eng (V14-V17-Interaction)\n",
            "2025-12-11 13:08:21\n",
            "\n",
            "| Configuration                           |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:----------------------------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 3.A + Feature Eng (V14-V17-Interaction) |            0.7306 | 0.0201 (2.8%)   | 0.0128 (1.7%)    |            0.7692 | 0.0169 (2.2%)   | 0.0141 (1.8%)    |          0.7073 | 0.0264 (3.7%) | 0.0117 (1.6%)  |             0.8455 | 0.0420 (5.0%)    | 0.0148 (1.7%)     |\n",
            "\n",
            "2025-12-11 13:08:21 Running Configuration: 4. + RUS (Calibrated)\n",
            "2025-12-11 13:08:28\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) | F2 Score (SD)   | F2 Score (Gap)   |   F1 Score (Mean) | F1 Score (SD)   | F1 Score (Gap)   |   Recall (Mean) | Recall (SD)   | Recall (Gap)   |   Precision (Mean) | Precision (SD)   | Precision (Gap)   |\n",
            "|:----------------------|------------------:|:----------------|:-----------------|------------------:|:----------------|:-----------------|----------------:|:--------------|:---------------|-------------------:|:-----------------|:------------------|\n",
            "| 4. + RUS (Calibrated) |            0.7377 | 0.0439 (5.9%)   | -0.0017 (-0.2%)  |            0.6662 | 0.0895 (13.4%)  | -0.0067 (-1.0%)  |          0.8008 | 0.0206 (2.6%) | 0.0040 (0.5%)  |             0.5839 | 0.1427 (24.4%)   | -0.0107 (-1.8%)   |\n",
            "\n",
            "2025-12-11 13:08:28 Running Configuration: 5. + Optimization (with RUS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-09 (5 repeat):\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration                           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)                   |            0.6195 |          0.0538 |            0.6599 |          0.0434 |             0.5959 |           0.0615 |                0.7465 |              0.0574 |\n",
        "| 2. + Scaling (Robust)                   |            0.6569 |          0.0464 |            0.7238 |          0.0364 |             0.6191 |           0.0516 |                0.8756 |              0.0299 |\n",
        "| 3.A + Feature Eng (Hour)                |            0.6576 |          0.0444 |            0.7249 |          0.0344 |             0.6195 |           0.0496 |                0.8778 |              0.0297 |\n",
        "| 3.A + Feature Eng (DayNight)            |            0.6561 |          0.0455 |            0.7228 |          0.0369 |             0.6183 |           0.05   |                0.8738 |              0.0317 |\n",
        "| 3.A + Feature Eng (LogAmount)           |            0.6599 |          0.046  |            0.7268 |          0.038  |             0.622  |           0.0501 |                0.8778 |              0.0329 |\n",
        "| 3.A + Feature Eng (V14-V17-Interaction) |            0.734  |          0.0326 |            0.7718 |          0.027  |             0.711  |           0.0378 |                0.8463 |              0.0382 |\n"
      ],
      "metadata": {
        "id": "LDeJcYviqfDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-09 (only 1 repeat):\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration                           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)                   |            0.6101 |          0.0708 |            0.6614 |          0.0504 |             0.5813 |           0.0815 |                0.7806 |              0.045  |\n",
        "| 2. + Scaling (Robust)                   |            0.6607 |          0.0378 |            0.729  |          0.0324 |             0.6221 |           0.0408 |                0.8827 |              0.0329 |\n",
        "| 3.A + Feature Eng (Hour)                |            0.657  |          0.0329 |            0.7261 |          0.0292 |             0.618  |           0.0351 |                0.8821 |              0.0329 |\n",
        "| 3.A + Feature Eng (DayNight)            |            0.6608 |          0.0326 |            0.7292 |          0.0287 |             0.622  |           0.0351 |                0.8828 |              0.0328 |\n",
        "| 3.A + Feature Eng (LogAmount)           |            0.6608 |          0.0316 |            0.7292 |          0.0286 |             0.6221 |           0.0336 |                0.8827 |              0.0334 |\n",
        "| 3.A + Feature Eng (V14-V17-Interaction) |            0.7306 |          0.0201 |            0.7692 |          0.0169 |             0.7073 |           0.0264 |                0.8455 |              0.042  |"
      ],
      "metadata": {
        "id": "BACN531bMvb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 with PozzoloCalibratedClassifier (after code changes):\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |   Best Params (Mean) |   Best Params (SD) |\n",
        "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|---------------------:|-------------------:|\n",
        "| 1. Naive (No Preproc)    |            0.6195 |          0.0538 |            0.6599 |          0.0434 |             0.5959 |           0.0615 |                0.7465 |              0.0574 |             nan      |           nan      |\n",
        "| 2. + Scaling (Robust)    |            0.6569 |          0.0464 |            0.7238 |          0.0364 |             0.6191 |           0.0516 |                0.8756 |              0.0299 |             nan      |           nan      |\n",
        "| 3. + Feature Eng (Hour)  |            0.6576 |          0.0444 |            0.7249 |          0.0344 |             0.6195 |           0.0496 |                0.8778 |              0.0297 |             nan      |           nan      |\n",
        "| 4. + RUS (Calibrated)    |            0.6978 |          0.0669 |            0.5995 |          0.1059 |             0.7967 |           0.0586 |                0.4988 |              0.1465 |             nan      |           nan      |\n",
        "| 5. + Optimization (Full) |            0.7008 |          0.0582 |            0.6215 |          0.0894 |             0.774  |           0.0573 |                0.5343 |              0.1329 |               0.5219 |             0.2571 |\n"
      ],
      "metadata": {
        "id": "GqhqVGVDypw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 with PozzoloCalibratedClassifier :\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean  SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195  0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569  0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576  0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978  0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.7008  0.0582        |     0.6215 |   0.774  |      0.5343 |\n"
      ],
      "metadata": {
        "id": "6AwBaeSVAYc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 without PozzoloCalibratedClassifier:\n",
        "\n",
        "\n",
        "```\n",
        "Dataset Shape: (284807, 30), Fraud Ratio: 0.1727%\n",
        "Starting Ablation Study (this may take a moment)...\n",
        "Running Configuration: 1. Naive (No Preproc)\n",
        "Running Configuration: 2. + Scaling (Robust)\n",
        "Running Configuration: 3. + Feature Eng (Hour)\n",
        "Running Configuration: 4. + RUS (Calibrated)\n",
        "Running Configuration: 5. + Optimization (Full)\n",
        "```\n",
        "\n",
        "\n",
        "=== Ablation Study Results (Baseline: Logistic Regression) ===\n",
        "| Configuration            | F2 Score (Mean  SD)   |   F1 Score |   Recall |   Precision |\n",
        "|:-------------------------|:-----------------------|-----------:|---------:|------------:|\n",
        "| 1. Naive (No Preproc)    | 0.6195  0.0538        |     0.6599 |   0.5959 |      0.7465 |\n",
        "| 2. + Scaling (Robust)    | 0.6569  0.0464        |     0.7238 |   0.6191 |      0.8756 |\n",
        "| 3. + Feature Eng (Hour)  | 0.6576  0.0444        |     0.7249 |   0.6195 |      0.8778 |\n",
        "| 4. + RUS (Calibrated)    | 0.6978  0.0669        |     0.5995 |   0.7967 |      0.4988 |\n",
        "| 5. + Optimization (Full) | 0.6052  0.0515        |     0.6251 |   0.5946 |      0.6721 |"
      ],
      "metadata": {
        "id": "Vz0aY15t47E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2D. Implementation of Classifier 1 - XGBoost\n"
      ],
      "metadata": {
        "id": "0MTNoylRlKM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint, uniform, loguniform\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGBoost Parameters: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "# Params version 1\n",
        "xgb_params_v1 = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': loguniform(0.01, 0.3)\n",
        "}\n",
        "\n",
        "# Params version 2\n",
        "xgb_params_v2 = {\n",
        "    'scale_pos_weight': randint(10, 101),  # Range: 10 - 100\n",
        "    'max_depth': randint(3, 7),  # Range: 3 - 6 (randint upper bound is exclusive)\n",
        "    'learning_rate': loguniform(0.01, 0.1),  # Range: 0.01 - 0.1\n",
        "    'n_estimators': [100, 200],\n",
        "    'min_child_weight': randint(5, 11),    # Range: 5 - 10\n",
        "    'gamma': uniform(0.1, 0.4),            # Range: 0.1 - 0.5 (loc=0.1, scale=0.4)\n",
        "    'subsample': uniform(0.6, 0.3),        # Range: 0.6 - 0.9 (loc=0.6, scale=0.3)\n",
        "    'colsample_bytree': uniform(0.6, 0.3)  # Range: 0.6 - 0.9 (loc=0.6, scale=0.3)\n",
        "}\n",
        "\n",
        "# Params version 3\n",
        "xgb_params_v3 = {\n",
        "    'scale_pos_weight': randint(10, 101),  # Range: 10 - 100\n",
        "    'max_depth': randint(3, 7),  # Range: 3 - 6 (randint upper bound is exclusive)\n",
        "    'learning_rate': loguniform(0.01, 0.1),  # Range: 0.01 - 0.1\n",
        "\n",
        "    'n_estimators': [150],\n",
        "    'min_child_weight': [5],\n",
        "    'gamma': [0.1],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "# Params version 4\n",
        "xgb_params_v4 = {\n",
        "    'scale_pos_weight': randint(10, 1000),  # Range including legit/fraud ratio 578:1\n",
        "    'max_depth': randint(3, 8), # Maximum depth of a tree. Increasing this value\n",
        "                                # will make the model more complex and more likely to overfit.\n",
        "    'learning_rate': loguniform(0.01, 0.1),  # default 0.3, Step size shrinkage used in update to prevent overfitting.\n",
        "\n",
        "    'n_estimators': [150],\n",
        "    #'min_child_weight': [5], # default 1, the larger, the more conservative\n",
        "    #'gamma': [0.1], # default 0, Minimum loss reduction required to make a\n",
        "                    # further partition on a leaf node of the tree. The larger\n",
        "                    # gamma is, the more conservative the algorithm will be.\n",
        "    #'subsample': [0.8], # default 1\n",
        "    #'colsample_bytree': [0.8] # default 1\n",
        "}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"XGBoost\",\n",
        "    base_clf=XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=1),\n",
        "      # eval_metric='logloss' - eval metric for learning, logloss (negative log-likelihood, suitable for classification)\n",
        "    param_dist=xgb_params_v4,\n",
        "    X=X, y=y, configs=configs\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: XGBoost) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NtDUHhClbo0",
        "outputId": "2a5447c5-9856-45ce-e0f0-b43afbd82f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Experiment: XGBoost ---\n",
            "\n",
            "2025-12-09 22:38:40 Running Configuration: 1. Naive (No Preproc)\n",
            "2025-12-09 22:40:55\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 1. Naive (No Preproc) |            0.7853 |          0.0285 |            0.8252 |          0.0276 |              0.761 |           0.0314 |                 0.903 |              0.0413 |\n",
            "\n",
            "2025-12-09 22:40:55 Running Configuration: 2. + Scaling (Robust)\n",
            "2025-12-09 22:43:16\n",
            "\n",
            "| Configuration         |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 2. + Scaling (Robust) |            0.7872 |          0.0295 |            0.8269 |          0.0288 |              0.763 |            0.032 |                 0.904 |              0.0423 |\n",
            "\n",
            "2025-12-09 22:43:16 Running Configuration: 3.A + Feature Eng (Hour)\n",
            "2025-12-09 22:45:42\n",
            "\n",
            "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 3.A + Feature Eng (Hour) |            0.7915 |           0.027 |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "\n",
            "2025-12-09 22:45:42 Running Configuration: 3.A + Feature Eng (DayNight)\n",
            "2025-12-09 22:48:14\n",
            "\n",
            "| Configuration                |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:-----------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 3.A + Feature Eng (DayNight) |            0.7915 |           0.027 |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "\n",
            "2025-12-09 22:48:14 Running Configuration: 3.A + Feature Eng (LogAmount)\n",
            "2025-12-09 22:50:53\n",
            "\n",
            "| Configuration                 |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 3.A + Feature Eng (LogAmount) |            0.7915 |           0.027 |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "\n",
            "2025-12-09 22:50:53 Running Configuration: 3.A + Feature Eng (V14-V17-Interaction)\n",
            "2025-12-09 22:53:41\n",
            "\n",
            "| Configuration                           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 3.A + Feature Eng (V14-V17-Interaction) |            0.7435 |          0.1628 |            0.7769 |          0.1746 |              0.723 |           0.1555 |                0.8413 |               0.198 |\n",
            "\n",
            "=== Ablation Study Results (Baseline: XGBoost) ===\n",
            "| Configuration                           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
            "|:----------------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
            "| 1. Naive (No Preproc)                   |            0.7853 |          0.0285 |            0.8252 |          0.0276 |             0.761  |           0.0314 |                0.903  |              0.0413 |\n",
            "| 2. + Scaling (Robust)                   |            0.7872 |          0.0295 |            0.8269 |          0.0288 |             0.763  |           0.032  |                0.904  |              0.0423 |\n",
            "| 3.A + Feature Eng (Hour)                |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "| 3.A + Feature Eng (DayNight)            |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "| 3.A + Feature Eng (LogAmount)           |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
            "| 3.A + Feature Eng (V14-V17-Interaction) |            0.7435 |          0.1628 |            0.7769 |          0.1746 |             0.723  |           0.1555 |                0.8413 |              0.198  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-09 (Params version 3, 5 repeat):\n",
        "\n",
        "=== Ablation Study Results (Baseline: XGBoost) ===\n",
        "| Configuration                           |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:----------------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)                   |            0.7853 |          0.0285 |            0.8252 |          0.0276 |             0.761  |           0.0314 |                0.903  |              0.0413 |\n",
        "| 2. + Scaling (Robust)                   |            0.7872 |          0.0295 |            0.8269 |          0.0288 |             0.763  |           0.032  |                0.904  |              0.0423 |\n",
        "| 3.A + Feature Eng (Hour)                |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
        "| 3.A + Feature Eng (DayNight)            |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
        "| 3.A + Feature Eng (LogAmount)           |            0.7915 |          0.027  |            0.8273 |          0.0272 |             0.7695 |           0.0295 |                0.8961 |              0.0429 |\n",
        "| 3.A + Feature Eng (V14-V17-Interaction) |            0.7435 |          0.1628 |            0.7769 |          0.1746 |             0.723  |           0.1555 |                0.8413 |              0.198  |\n"
      ],
      "metadata": {
        "id": "952Wm49qrF0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-08 (Params version 3, only 1 repeat):\n",
        "\n",
        "=== Ablation Study Results (Baseline: XGBoost) ===\n",
        "| Configuration                   |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:--------------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)           |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                0.897  |              0.0386 |\n",
        "| 2. + Scaling (Robust)           |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
        "| 3. + Feature Eng (Hour)         |            0.7959 |          0.0399 |            0.8342 |          0.0413 |             0.7723 |           0.0399 |                0.9076 |              0.0495 |\n",
        "| 4. + RUS (Calibrated)           |            0.7563 |          0.0367 |            0.7234 |          0.0781 |             0.7846 |           0.0169 |                0.6841 |              0.1264 |\n",
        "| 5. + Optimization (with RUS)    |            0.781  |          0.0284 |            0.7696 |          0.0566 |             0.7907 |           0.0215 |                0.7574 |              0.105  |\n",
        "| 6. + Optimization (without RUS) |            0.8403 |          0.0193 |            0.8576 |          0.0273 |             0.8293 |           0.0164 |                0.889  |              0.0458 |"
      ],
      "metadata": {
        "id": "JbxB5NRmtAQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VC_ruGSvD2El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025-11-06 (Params version 1, only 2 repeats):\n",
        "\n",
        "=== Ablation Study Results (Baseline: XGBoost) ===\n",
        "| Configuration            |   F2 Score (Mean) |   F2 Score (SD) |   F1 Score (Mean) |   F1 Score (SD) |   F2 Recall (Mean) |   F2 Recall (SD) |   F1 Precision (Mean) |   F1 Precision (SD) |\n",
        "|:-------------------------|------------------:|----------------:|------------------:|----------------:|-------------------:|-----------------:|----------------------:|--------------------:|\n",
        "| 1. Naive (No Preproc)    |            0.7925 |          0.0279 |            0.8285 |          0.0282 |             0.7704 |           0.0293 |                0.897  |              0.0386 |\n",
        "| 2. + Scaling (Robust)    |            0.7929 |          0.0284 |            0.8294 |          0.0293 |             0.7704 |           0.0293 |                0.8991 |              0.0395 |\n",
        "| 3. + Feature Eng (Hour)  |            0.7995 |          0.0253 |            0.8371 |          0.0294 |             0.7765 |           0.0246 |                0.9088 |              0.0443 |\n",
        "| 4. + RUS (Calibrated)    |            0.7695 |          0.0426 |            0.7339 |          0.0791 |             0.7988 |           0.0148 |                0.6885 |              0.1192 |\n",
        "| 5. + Optimization (Full) |            0.7619 |          0.0351 |            0.7321 |          0.0698 |             0.7866 |           0.0275 |                0.6959 |              0.1219 |\n"
      ],
      "metadata": {
        "id": "ikAouZIC9RZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2D. Implementation of Classifier 2 - XGBOD\n"
      ],
      "metadata": {
        "id": "sUeotW2El1Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "mfRCu6RuE4bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOD is essentially XGBoost + TOS Features\n",
        "# So we use the same base_clf and params as XGBoost, but add the feature generator\n",
        "\n",
        "# XGBoost Parameters: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': loguniform(0.01, 0.3)\n",
        "}\n",
        "\n",
        "df_results = run_ablation_study(\n",
        "    experiment_name=\"XGBOD (XGBoost + TOS)\",\n",
        "    base_clf=XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=1),\n",
        "      # eval_metric='logloss' - eval metric for learning, logloss (negative log-likelihood, suitable for classification)\n",
        "    param_dist=xgb_params,\n",
        "    X=X, y=y, configs=configs,\n",
        "    extra_pipeline_steps=[('tos', TOSGenerator())] # <--- This injects the Outlier Scores\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ablation Study Results (Baseline: XGBOD) ===\")\n",
        "print(df_results.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Q1XubgObmFC5",
        "outputId": "792aa782-157a-4dde-9f97-8d76c24fa365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TOSGenerator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-225491100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mparam_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mextra_pipeline_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOSGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# <--- This injects the Outlier Scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TOSGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We define distinct pipelines to test each component\n",
        "configs = {\n",
        "    \"1. Naive (No Preproc)\": {\n",
        "        'scale': False, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"2. + Scaling (Robust)\": {\n",
        "        'scale': True, 'fe': False, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"3. + Feature Eng (Hour)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': False\n",
        "    },\n",
        "    \"4. + RUS (Calibrated)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': False\n",
        "    },\n",
        "    \"5. + Optimization (with RUS)\": {\n",
        "        'scale': True, 'fe': True, 'rus': True, 'opt': True\n",
        "    },\n",
        "    \"6. + Optimization (without RUS)\": {\n",
        "        'scale': True, 'fe': True, 'rus': False, 'opt': True\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "T6L-kS5bFTac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FraudFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Comprehensive Feature Engineering for Credit Card Fraud.\n",
        "\n",
        "    Adds:\n",
        "    1. Time Features: Hour, Is_Night\n",
        "    2. Amount Features: Log_Amount, Amount_Decimal\n",
        "    3. Interaction Features: V14 * V17 (based on EDA correlations)\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Applies transformations to X.\"\"\"\n",
        "        X_out = X.copy()\n",
        "\n",
        "        # --- 1. Time Features ---\n",
        "        if 'Time' in X_out.columns:\n",
        "            # Hour (0-23)\n",
        "            X_out['Hour'] = (X_out['Time'] % 86400) // 3600\n",
        "\n",
        "            # Is_Night (Binary): 1 if between 22:00 and 06:00\n",
        "            # Helping trees isolate nocturnal fraud patterns instantly\n",
        "            X_out['Is_Night'] = (X_out['Hour'] >= 22) | (X_out['Hour'] <= 6)\n",
        "            X_out['Is_Night'] = X_out['Is_Night'].astype(int)\n",
        "\n",
        "        # --- 2. Amount Features ---\n",
        "        if 'Amount' in X_out.columns:\n",
        "            # Log Amount: Handles the extreme skew of transaction amounts\n",
        "            # np.log1p avoids log(0) errors\n",
        "            X_out['Log_Amount'] = np.log1p(X_out['Amount'])\n",
        "\n",
        "            # Decimal Part: Captures pricing psychology (e.g., 9.99 vs 10.00)\n",
        "            X_out['Amount_Dec'] = X_out['Amount'] % 1\n",
        "\n",
        "        # --- 3. Interaction Features (Based on EDA) ---\n",
        "        # V17 and V14 had the strongest correlations in your EDA.\n",
        "        # Multiplying them helps trees find the \"diagonal\" decision boundary.\n",
        "        if 'V14' in X_out.columns and 'V17' in X_out.columns:\n",
        "            X_out['V14_V17'] = X_out['V14'] * X_out['V17']\n",
        "\n",
        "        return X_out"
      ],
      "metadata": {
        "id": "KZq1yah71Jzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}