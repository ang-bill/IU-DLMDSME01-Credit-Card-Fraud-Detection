{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfzy3EuK7kKUKRVRt6RXAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ang-bill/IU-DLMDSME01-Credit-Card-Fraud-Detection/blob/main/Task1_Credit_Card_Fraud_Detection_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Classifier 1"
      ],
      "metadata": {
        "id": "C9SieIYotZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A. Retrieve Dataset from Kaggle Hub\n",
        "At the first run, the dataset is downloaded from Kaggle and stored locally. Subsequent runs check whether the file already exists.\n",
        "See: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
      ],
      "metadata": {
        "id": "nWdKMlMnrOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28d_gYwkpQn",
        "outputId": "7a460108-c185-4d36-bb6e-6bc280cd19df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'creditcard.csv' found locally at '/content/drive/MyDrive/Colab_Kaggle_Data/mlg-ulb/creditcardfraud/creditcard.csv'. Loading from there.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd # Pandas dataframe\n",
        "import kagglehub # Kagglehub to access dataset\n",
        "import shutil # Util for copying files\n",
        "from google.colab import drive # Import Google Drive utilities\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "local_storage_base_dir = \"/content/drive/MyDrive/Colab_Kaggle_Data\"\n",
        "\n",
        "# Dataset details\n",
        "kaggle_dataset_id = \"mlg-ulb/creditcardfraud\"\n",
        "file_name_in_dataset = \"creditcard.csv\"\n",
        "\n",
        "# Construct the full path to locally stored dataset\n",
        "local_dataset_dir = os.path.join(local_storage_base_dir, *kaggle_dataset_id.split('/'))\n",
        "full_local_file_path = os.path.join(local_dataset_dir, file_name_in_dataset)\n",
        "\n",
        "# Ensure the desired local storage directory exists\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "df = None # Initialize pandas df\n",
        "\n",
        "# Check if the file already exists in local storage, otherwise download from Kaggle\n",
        "if os.path.exists(full_local_file_path):\n",
        "    print(f\"'{file_name_in_dataset}' found locally at '{full_local_file_path}'. Loading from there.\")\n",
        "else:\n",
        "    print(f\"'{file_name_in_dataset}' not found locally. Attempting to download from KaggleHub and store it.\")\n",
        "\n",
        "    # Use kagglehub.dataset_download to get the dataset.\n",
        "    downloaded_source_root = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "\n",
        "    # Construct the path to the file within the KaggleHub download location\n",
        "    source_file_path = os.path.join(downloaded_source_root, file_name_in_dataset)\n",
        "\n",
        "    if os.path.exists(source_file_path):\n",
        "        print(f\"Dataset found at KaggleHub resolved location: '{source_file_path}'.\")\n",
        "        print(f\"Copying '{file_name_in_dataset}' to local path: '{full_local_file_path}'.\")\n",
        "\n",
        "        # Copy the file to local storage location\n",
        "        shutil.copy(source_file_path, full_local_file_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Failed to find '{file_name_in_dataset}' at source '{source_file_path}' after KaggleHub download resolution.\")\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv(full_local_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B. Implementation of Classifier 1\n"
      ],
      "metadata": {
        "id": "dulJshxnkvMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# 1. SETUP: Generate synthetic imbalanced data for demonstration\n",
        "# Imagine this is your fraud dataset: 10,000 rows, only 1% anomalies\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, weights=[0.99, 0.01], random_state=42)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Original Training Distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PHASE 1: Unsupervised Representation Learning (Generating TOS)\n",
        "# ==============================================================================\n",
        "# We train unsupervised detectors on the FULL imbalanced X_train.\n",
        "# They need the density of the majority class to understand what is \"normal.\"\n",
        "\n",
        "print(\"Step 1: Generating Transformed Outlier Scores (TOS)...\")\n",
        "\n",
        "# Define the unsupervised detectors (Components of XGBOD)\n",
        "detectors = [KNN(n_neighbors=5), LOF(n_neighbors=5), KNN(n_neighbors=20)]\n",
        "X_train_tos_list = []\n",
        "X_test_tos_list = []\n",
        "\n",
        "for clf in detectors:\n",
        "    clf.fit(X_train) # Fit on full data\n",
        "\n",
        "    # Get outlier scores (decision_scores_ usually returns raw anomaly scores)\n",
        "    # We reshape to (n_samples, 1) to stack them later\n",
        "    train_scores = clf.decision_scores_.reshape(-1, 1)\n",
        "    test_scores = clf.decision_function(X_test).reshape(-1, 1)\n",
        "\n",
        "    X_train_tos_list.append(train_scores)\n",
        "    X_test_tos_list.append(test_scores)\n",
        "\n",
        "# Concatenate original features with new TOS features\n",
        "X_train_augmented = np.hstack([X_train] + X_train_tos_list)\n",
        "X_test_augmented = np.hstack([X_test] + X_test_tos_list)\n",
        "\n",
        "print(f\"Feature Space Augmented: {X_train.shape[1]} -> {X_train_augmented.shape[1]} features\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PHASE 2: The \"Inter-Phase\" (Applying RUS)\n",
        "# ==============================================================================\n",
        "# NOW we apply Random Undersampling.\n",
        "# We do this AFTER the outlier scores are generated, but BEFORE the classifier learns.\n",
        "\n",
        "print(\"Step 2: Applying Random Undersampling (RUS)...\")\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42, sampling_strategy=1.0) # 1:1 ratio\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train_augmented, y_train)\n",
        "\n",
        "print(f\"Resampled Training Distribution: {np.bincount(y_train_rus)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PHASE 3: Supervised Learning (XGBoost)\n",
        "# ==============================================================================\n",
        "# Train the final classifier on the balanced, feature-rich data\n",
        "\n",
        "print(\"Step 3: Training XGBoost...\")\n",
        "\n",
        "# Note: scale_pos_weight is usually not needed if we use RUS to 1:1 balance,\n",
        "# but checking hyperparams is always good.\n",
        "xgb = XGBClassifier(n_estimators=100, max_depth=3, eval_metric='logloss')\n",
        "xgb.fit(X_train_rus, y_train_rus)\n",
        "\n",
        "# ==============================================================================\n",
        "# EVALUATION (F2 Score)\n",
        "# ==============================================================================\n",
        "# Predict on the test set (which is still imbalanced and un-sampled!)\n",
        "y_pred = xgb.predict(X_test_augmented)\n",
        "\n",
        "# Calculate F2 Score (beta=2 favors Recall)\n",
        "f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "print(f\"--- Results ---\")\n",
        "print(f\"Final F2-Score: {f2:.4f}\")"
      ],
      "metadata": {
        "id": "GQbc-fm_kq8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}